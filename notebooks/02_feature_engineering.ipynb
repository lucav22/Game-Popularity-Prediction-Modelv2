{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Feature Engineering for Game Popularity Prediction\n",
    "\n",
    "**Purpose:** This notebook demonstrates advanced feature engineering techniques to extract meaningful patterns from raw player count data for our game popularity prediction model.\n",
    "\n",
    "**Why This Matters:** Raw player counts alone don't capture the nuances of game popularity. Through feature engineering, we extract:\n",
    "- Trend and seasonality patterns\n",
    "- Player retention metrics\n",
    "- Growth/decline acceleration\n",
    "- Comparative performance metrics\n",
    "\n",
    "**Expected Outcomes:**\n",
    "1. Transformed raw data into informative features\n",
    "2. Identified key patterns in game lifecycle stages\n",
    "3. Created features ready for predictive modeling\n",
    "4. Validated feature quality and importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "**Purpose:** Initialize environment and load historical data for feature engineering.\n",
    "\n",
    "**Why:** We need a robust dataset with multiple time points to create meaningful time-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src directory to path\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Import our custom modules\n",
    "from data_collector import DataCollector\n",
    "from utils import (\n",
    "    configure_plotting, format_large_numbers, \n",
    "    calculate_retention_metrics, analyze_temporal_patterns,\n",
    "    detect_anomalies\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "configure_plotting()\n",
    "\n",
    "# Display start time\n",
    "print(f\"Feature Engineering Started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Historical Data\n",
    "\n",
    "**Purpose:** Load all collected data and understand its structure.\n",
    "\n",
    "**What to Expect:** A consolidated time series dataset with player counts across all tracked games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collector\n",
    "collector = DataCollector(data_dir=\"../data\")\n",
    "\n",
    "# Load all historical data\n",
    "historical_data = collector.merge_historical_data()\n",
    "\n",
    "# Display data summary\n",
    "print(\"Historical Data Summary:\")\n",
    "print(f\"Total records: {len(historical_data)}\")\n",
    "print(f\"Date range: {historical_data['timestamp'].min()} to {historical_data['timestamp'].max()}\")\n",
    "print(f\"Unique games: {historical_data['app_id'].nunique()}\")\n",
    "\n",
    "# Show data sample\n",
    "print(\"\\nData sample:\")\n",
    "display(historical_data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values by column:\")\n",
    "missing_values = historical_data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Series Feature Engineering\n",
    "\n",
    "**Purpose:** Extract time-based features from player count data.\n",
    "\n",
    "**Why:** Time series patterns reveal game health, lifecycle stages, and predict future performance.\n",
    "\n",
    "**Features Created:**\n",
    "- Rolling averages (7-day, 30-day)\n",
    "- Trend indicators\n",
    "- Volatility measures\n",
    "- Growth rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create time series features for a single game\n",
    "def create_time_features(game_df, player_col='player_count'):\n",
    "    \"\"\"Create time-based features for a game's player data\"\"\"\n",
    "    \n",
    "    # Ensure data is sorted by timestamp\n",
    "    game_df = game_df.sort_values('timestamp')\n",
    "    \n",
    "    # Calculate rolling averages\n",
    "    game_df['player_7d_avg'] = game_df[player_col].rolling(window=7, min_periods=1).mean()\n",
    "    game_df['player_30d_avg'] = game_df[player_col].rolling(window=30, min_periods=1).mean()\n",
    "    \n",
    "    # Calculate growth rates\n",
    "    game_df['growth_1d'] = game_df[player_col].pct_change()\n",
    "    game_df['growth_7d'] = game_df['player_7d_avg'].pct_change(periods=7)\n",
    "    game_df['growth_30d'] = game_df['player_30d_avg'].pct_change(periods=30)\n",
    "    \n",
    "    # Calculate acceleration (rate of change of growth)\n",
    "    game_df['acceleration'] = game_df['growth_7d'].diff()\n",
    "    \n",
    "    # Calculate volatility (standard deviation of recent changes)\n",
    "    game_df['volatility_7d'] = game_df['growth_1d'].rolling(window=7).std()\n",
    "    game_df['volatility_30d'] = game_df['growth_1d'].rolling(window=30).std()\n",
    "    \n",
    "    # Calculate momentum indicators\n",
    "    game_df['momentum_7d'] = game_df[player_col] - game_df['player_7d_avg']\n",
    "    game_df['momentum_30d'] = game_df[player_col] - game_df['player_30d_avg']\n",
    "    \n",
    "    # Calculate relative strength index (RSI) - simplified version\n",
    "    delta = game_df[player_col].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    \n",
    "    rs = gain / loss\n",
    "    game_df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    return game_df\n",
    "\n",
    "# Apply time features to all games\n",
    "enriched_data = []\n",
    "\n",
    "for app_id in historical_data['app_id'].unique():\n",
    "    game_data = historical_data[historical_data['app_id'] == app_id].copy()\n",
    "    if len(game_data) >= 7:  # Need at least 7 days of data\n",
    "        game_features = create_time_features(game_data)\n",
    "        enriched_data.append(game_features)\n",
    "\n",
    "# Combine all games\n",
    "feature_data = pd.concat(enriched_data, ignore_index=True)\n",
    "\n",
    "print(f\"Time series features created for {len(enriched_data)} games\")\n",
    "print(\"\\nNew feature columns:\")\n",
    "new_columns = [col for col in feature_data.columns if col not in historical_data.columns]\n",
    "for col in new_columns:\n",
    "    print(f\" - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retention and Lifecycle Features\n",
    "\n",
    "**Purpose:** Calculate game lifecycle stages and retention metrics.\n",
    "\n",
    "**Why:** Understanding where a game is in its lifecycle helps predict its future trajectory.\n",
    "\n",
    "**Features Created:**\n",
    "- Days since release\n",
    "- Lifecycle stage classification\n",
    "- Retention curves\n",
    "- Peak performance indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lifecycle features\n",
    "def add_lifecycle_features(df):\n",
    "    \"\"\"Add lifecycle-related features to game data\"\"\"\n",
    "    \n",
    "    # Convert release date to datetime\n",
    "    df['release_datetime'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
    "    \n",
    "    # Calculate days since release\n",
    "    df['days_since_release'] = (df['timestamp'] - df['release_datetime']).dt.days\n",
    "    \n",
    "    # Define lifecycle stages based on days since release\n",
    "    def classify_lifecycle_stage(days):\n",
    "        if pd.isna(days):\n",
    "            return 'unknown'\n",
    "        elif days < 30:\n",
    "            return 'launch'\n",
    "        elif days < 180:\n",
    "            return 'early'\n",
    "        elif days < 365:\n",
    "            return 'growth'\n",
    "        elif days < 730:\n",
    "            return 'mature'\n",
    "        else:\n",
    "            return 'established'\n",
    "    \n",
    "    df['lifecycle_stage'] = df['days_since_release'].apply(classify_lifecycle_stage)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply lifecycle features\n",
    "feature_data = add_lifecycle_features(feature_data)\n",
    "\n",
    "# Calculate game-level metrics\n",
    "game_metrics = feature_data.groupby('app_id').agg({\n",
    "    'player_count': ['max', 'mean', 'std', 'count'],\n",
    "    'days_since_release': 'max',\n",
    "    'lifecycle_stage': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'unknown',\n",
    "    'category': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'unknown'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "game_metrics.columns = ['app_id', 'peak_players', 'avg_players', 'player_volatility', \n",
    "                       'data_points', 'current_age_days', 'lifecycle_stage', 'category']\n",
    "\n",
    "# Calculate retention metrics\n",
    "retention_metrics = calculate_retention_metrics(feature_data)\n",
    "\n",
    "# Merge metrics\n",
    "game_metrics = game_metrics.merge(retention_metrics, on='app_id', how='left', suffixes=('', '_ret'))\n",
    "\n",
    "print(\"Lifecycle Features Added\")\n",
    "print(f\"Games by lifecycle stage:\")\n",
    "stage_counts = feature_data['lifecycle_stage'].value_counts()\n",
    "for stage, count in stage_counts.items():\n",
    "    print(f\" - {stage}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparative Features\n",
    "\n",
    "**Purpose:** Create features that compare game performance to peers and benchmarks.\n",
    "\n",
    "**Why:** Relative performance often matters more than absolute numbers in predicting future success.\n",
    "\n",
    "**Features Created:**\n",
    "- Category rankings\n",
    "- Percentile scores\n",
    "- Competitive positioning\n",
    "- Genre performance ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate competitive features\n",
    "def add_competitive_features(df):\n",
    "    \"\"\"Add features comparing games to their category peers\"\"\"\n",
    "    \n",
    "    # Calculate percentile ranks within category\n",
    "    for category in df['category'].unique():\n",
    "        mask = df['category'] == category\n",
    "        df.loc[mask, 'category_rank'] = df.loc[mask, 'player_count'].rank(pct=True)\n",
    "        \n",
    "        # Calculate distance from category average\n",
    "        category_avg = df.loc[mask, 'player_count'].mean()\n",
    "        df.loc[mask, 'category_distance'] = (df.loc[mask, 'player_count'] - category_avg) / category_avg\n",
    "    \n",
    "    # Calculate genre dominance (assuming we have genre data)\n",
    "    if 'genres' in df.columns:\n",
    "        # Extract primary genre for simplicity\n",
    "        df['primary_genre'] = df['genres'].str.split(',').str[0].fillna('Unknown')\n",
    "        \n",
    "        # Calculate genre market share\n",
    "        genre_totals = df.groupby('primary_genre')['player_count'].sum()\n",
    "        df['genre_total_players'] = df['primary_genre'].map(genre_totals)\n",
    "        df['genre_market_share'] = df['player_count'] / df['genre_total_players']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply competitive features\n",
    "feature_data = add_competitive_features(feature_data)\n",
    "\n",
    "# Calculate overall market position\n",
    "feature_data['overall_rank'] = feature_data['player_count'].rank(pct=True)\n",
    "\n",
    "# Create competitive intensity feature\n",
    "def calculate_competitive_intensity(df):\n",
    "    \"\"\"Calculate how competitive the game's space is\"\"\"\n",
    "    \n",
    "    # Count games with similar player counts\n",
    "    player_ranges = {\n",
    "        'micro': (0, 100),\n",
    "        'small': (100, 1000),\n",
    "        'medium': (1000, 10000),\n",
    "        'large': (10000, 100000),\n",
    "        'massive': (100000, float('inf'))\n",
    "    }\n",
    "    \n",
    "    def get_player_scale(count):\n",
    "        for scale, (min_val, max_val) in player_ranges.items():\n",
    "            if min_val <= count < max_val:\n",
    "                return scale\n",
    "        return 'unknown'\n",
    "    \n",
    "    df['player_scale'] = df['player_count'].apply(get_player_scale)\n",
    "    \n",
    "    # Count competitors in same scale\n",
    "    scale_counts = df.groupby('player_scale').size()\n",
    "    df['scale_competitors'] = df['player_scale'].map(scale_counts)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply competitive intensity\n",
    "feature_data = calculate_competitive_intensity(feature_data)\n",
    "\n",
    "print(\"Competitive Features Added\")\n",
    "print(\"\\nPlayer Scale Distribution:\")\n",
    "scale_dist = feature_data['player_scale'].value_counts()\n",
    "for scale, count in scale_dist.items():\n",
    "    print(f\" - {scale}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Seasonal and Cyclic Features\n",
    "\n",
    "**Purpose:** Extract seasonal patterns and recurring cycles in player behavior.\n",
    "\n",
    "**Why:** Understanding natural cycles helps distinguish between normal fluctuations and significant changes.\n",
    "\n",
    "**Features Created:**\n",
    "- Day of week effects\n",
    "- Monthly seasonality\n",
    "- Holiday impacts\n",
    "- Weekend/weekday patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time-based cyclic features\n",
    "def add_cyclic_features(df):\n",
    "    \"\"\"Add seasonal and cyclic features to the data\"\"\"\n",
    "    \n",
    "    # Extract time components\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    \n",
    "    # Create cyclic encoding for periodic features\n",
    "    # Day of week (cyclical)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Month (cyclical)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # Hour (cyclical)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply cyclic features\n",
    "feature_data = add_cyclic_features(feature_data)\n",
    "\n",
    "# Analyze weekend effect\n",
    "weekend_effect = feature_data.groupby('is_weekend')['player_count'].mean()\n",
    "print(\"Weekend Effect Analysis:\")\n",
    "print(f\"Weekday average: {weekend_effect[0]:.0f} players\")\n",
    "print(f\"Weekend average: {weekend_effect[1]:.0f} players\")\n",
    "print(f\"Weekend boost: {(weekend_effect[1] / weekend_effect[0] - 1) * 100:.1f}%\")\n",
    "\n",
    "# Visualize day-of-week patterns\n",
    "plt.figure(figsize=(12, 6))\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_patterns = feature_data.groupby('day_of_week')['player_count'].mean()\n",
    "day_patterns.index = day_names\n",
    "\n",
    "sns.barplot(x=day_patterns.index, y=day_patterns.values)\n",
    "plt.title('Average Player Count by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Average Player Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Player Engagement Quality Features\n",
    "\n",
    "**Purpose:** Create features that measure engagement quality beyond simple player counts.\n",
    "\n",
    "**Why:** Steady engagement often indicates healthier games than just high peak numbers.\n",
    "\n",
    "**Features Created:**\n",
    "- Engagement stability index\n",
    "- Player base maturity\n",
    "- Growth sustainability metrics\n",
    "- Recovery patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate engagement quality features\n",
    "def calculate_engagement_quality(df, app_id):\n",
    "    \"\"\"Calculate engagement quality metrics for a game\"\"\"\n",
    "    \n",
    "    game_data = df[df['app_id'] == app_id].sort_values('timestamp')\n",
    "    \n",
    "    if len(game_data) < 14:  # Need sufficient data\n",
    "        return {}\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Engagement stability (inverse of coefficient of variation)\n",
    "    cv = game_data['player_count'].std() / game_data['player_count'].mean()\n",
    "    metrics['engagement_stability'] = 1 / (1 + cv)\n",
    "    \n",
    "    # Growth consistency (how often growth is positive)\n",
    "    growth_positive_rate = (game_data['growth_1d'] > 0).mean()\n",
    "    metrics['growth_consistency'] = growth_positive_rate\n",
    "    \n",
    "    # Recovery ability (ability to bounce back after drops)\n",
    "    drops = game_data[game_data['growth_1d'] < -0.05]  # significant drops\n",
    "    if len(drops) > 0:\n",
    "        recoveries = []\n",
    "        for idx in drops.index:\n",
    "            future_data = game_data[game_data.index > idx][:7]  # next 7 days\n",
    "            if len(future_data) > 0:\n",
    "                recovery_rate = future_data['growth_1d'].mean()\n",
    "                recoveries.append(recovery_rate)\n",
    "        \n",
    "        metrics['recovery_ability'] = np.mean(recoveries) if recoveries else 0\n",
    "    else:\n",
    "        metrics['recovery_ability'] = 0\n",
    "    \n",
    "    # Player base maturity (days with stable player count)\n",
    "    stable_threshold = 0.02  # Â±2% change considered stable\n",
    "    stable_days = (abs(game_data['growth_1d']) < stable_threshold).sum()\n",
    "    metrics['player_base_maturity'] = stable_days / len(game_data)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate engagement quality for all games\n",
    "engagement_metrics = []\n",
    "\n",
    "for app_id in feature_data['app_id'].unique():\n",
    "    metrics = calculate_engagement_quality(feature_data, app_id)\n",
    "    if metrics:\n",
    "        metrics['app_id'] = app_id\n",
    "        engagement_metrics.append(metrics)\n",
    "\n",
    "engagement_df = pd.DataFrame(engagement_metrics)\n",
    "\n",
    "# Merge with game information\n",
    "if len(engagement_df) > 0:\n",
    "    game_names = feature_data.groupby('app_id')['name'].first().reset_index()\n",
    "    engagement_df = engagement_df.merge(game_names, on='app_id', how='left')\n",
    "    \n",
    "    print(\"Engagement Quality Metrics Calculated\")\n",
    "    print(\"\\nTop 5 Games by Engagement Stability:\")\n",
    "    top_stable = engagement_df.nlargest(5, 'engagement_stability')\n",
    "    for _, row in top_stable.iterrows():\n",
    "        print(f\"{row['name']}: {row['engagement_stability']:.3f}\")\n",
    "    \n",
    "    # Visualize engagement quality distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Stability distribution\n",
    "    sns.histplot(engagement_df['engagement_stability'], bins=20, ax=axes[0,0])\n",
    "    axes[0,0].set_title('Engagement Stability Distribution')\n",
    "    axes[0,0].set_xlabel('Stability Score')\n",
    "    \n",
    "    # Growth consistency\n",
    "    sns.histplot(engagement_df['growth_consistency'], bins=20, ax=axes[0,1])\n",
    "    axes[0,1].set_title('Growth Consistency Distribution')\n",
    "    axes[0,1].set_xlabel('Consistency Score')\n",
    "    \n",
    "    # Recovery ability\n",
    "    sns.histplot(engagement_df['recovery_ability'], bins=20, ax=axes[1,0])\n",
    "    axes[1,0].set_title('Recovery Ability Distribution')\n",
    "    axes[1,0].set_xlabel('Recovery Score')\n",
    "    \n",
    "    # Player base maturity\n",
    "    sns.histplot(engagement_df['player_base_maturity'], bins=20, ax=axes[1,1])\n",
    "    axes[1,1].set_title('Player Base Maturity Distribution')\n",
    "    axes[1,1].set_xlabel('Maturity Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Selection and Validation\n",
    "\n",
    "**Purpose:** Select the most important features and validate their quality.\n",
    "\n",
    "**Why:** Not all engineered features are equally valuable. Selecting the right features improves model performance.\n",
    "\n",
    "**Methods Used:**\n",
    "- Correlation analysis\n",
    "- Feature importance evaluation\n",
    "- Multicollinearity check\n",
    "- Domain knowledge validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature set for analysis\n",
    "feature_columns = [\n",
    "    'player_count', 'player_7d_avg', 'player_30d_avg',\n",
    "    'growth_1d', 'growth_7d', 'growth_30d',\n",
    "    'volatility_7d', 'volatility_30d',\n",
    "    'momentum_7d', 'momentum_30d',\n",
    "    'rsi', 'days_since_release',\n",
    "    'category_rank', 'overall_rank',\n",
    "    'is_weekend', 'day_of_week', 'month'\n",
    "]\n",
    "\n",
    "# Create feature matrix\n",
    "X = feature_data[feature_columns].copy()\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.2f', square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features\n",
    "high_correlation_pairs = []\n",
    "threshold = 0.8\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",