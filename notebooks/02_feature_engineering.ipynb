{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5185f9bd",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# 02. Feature Engineering and Data Aggregation for Predictive Modeling\n",
    "\n",
    "**Project Context:** This notebook is the second stage in a multi-part project aimed at predicting game popularity. It builds upon the raw, time-stamped data systematically gathered by the `01_data_collection.ipynb` notebook.\n",
    "\n",
    "**Purpose of this Notebook:** The primary objective here is to transform the granular, longitudinal data into a consolidated, feature-rich dataset suitable for machine learning applications. This involves leveraging a custom `DataAggregator` class to:\n",
    "*   Consolidate data from multiple collection instances.\n",
    "*   Calculate aggregated metrics that represent game performance and community engagement over specific time windows, particularly distinguishing between pre-release hype and post-launch success indicators.\n",
    "*   Structure the data such that each game is represented by a single row, with columns corresponding to engineered features and potential target variables.\n",
    "\n",
    "**Methodological Significance:** Raw time-series data, while rich in detail, is often not directly amenable to predictive models aiming to forecast a singular outcome (e.g., peak player count within a certain period post-launch). Feature engineering is crucial for extracting salient signals by aggregating data over relevant intervals. For instance, metrics like average social media sentiment in the weeks leading up to a game's release can serve as 'pre-release features,' while metrics like peak player counts or average viewership in the initial weeks post-launch can act as 'post-launch outcomes' or target variables.\n",
    "\n",
    "**Expected Outcomes:** Upon successful execution of this notebook, the following will be achieved:\n",
    "1.  **Consolidated Raw Data:** All individual data files generated by `01_data_collection.ipynb` will be loaded and merged into a unified DataFrame.\n",
    "2.  **Feature Aggregation:** The `DataAggregator` will process this merged data, calculating various features and outcome metrics for each game based on its release date and predefined aggregation windows.\n",
    "3.  **Structured Feature Set:** A new DataFrame will be produced where each row corresponds to a unique game. This DataFrame will contain columns representing:\n",
    "    *   Static game information (e.g., App ID, name, release date, Metacritic score).\n",
    "    *   Aggregated pre-release metrics (e.g., average Google Trends score, Reddit activity, YouTube views/likes before launch).\n",
    "    *   Aggregated post-launch metrics (e.g., peak Steam player count, peak Twitch viewers, average engagement metrics after launch).\n",
    "4.  **Preliminary Analysis and Cleaning:** Initial data cleaning steps (e.g., handling missing values) and exploratory analysis (e.g., descriptive statistics, correlation analysis) will be performed on the aggregated feature set.\n",
    "5.  **Persistent Storage:** The final, aggregated feature set will be saved to a CSV file, ready for use in the subsequent modeling phase (`03_modeling.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca777c5",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 1. Environment Initialization and Library Imports\n",
    "\n",
    "**Purpose:** This initial section is dedicated to establishing the necessary Python environment for the feature engineering and data aggregation tasks. It involves importing all required libraries and configuring system paths to ensure custom modules are discoverable.\n",
    "\n",
    "**Key Actions Undertaken:**\n",
    "*   **Standard Library Imports:** Essential Python libraries for data manipulation, numerical operations, and visualization are imported. This typically includes:\n",
    "    *   `sys` and `os`: For system-level operations, primarily path manipulation.\n",
    "    *   `pandas`: For DataFrame creation, manipulation, and analysis.\n",
    "    *   `numpy`: For numerical computations, especially array operations.\n",
    "    *   `matplotlib.pyplot` and `seaborn`: For creating static, interactive, and informative statistical graphics.\n",
    "    *   `datetime`: For handling and recording timestamps, particularly the notebook's execution start time.\n",
    "*   **Path Configuration for Custom Modules:** The Python system path (`sys.path`) is dynamically augmented to include the project's `src` directory (e.g., `../src/`). This step is critical for enabling the import of custom-developed modules, most notably the `DataAggregator` class, which is central to this notebook's functionality.\n",
    "*   **Custom Module Import:** The `DataAggregator` class is imported from the `src.aggregator` module. This class encapsulates the core logic for loading, merging, and aggregating the time-series data collected in the previous notebook.\n",
    "*   **Visualization Styling (Optional):** Plotting styles (e.g., `seaborn-v0_8-whitegrid`) can be set to ensure consistent and aesthetically pleasing visualizations throughout the notebook.\n",
    "*   **Pandas Display Configuration:** `pandas` display options are configured (e.g., `display.max_columns`, `display.max_rows`, `display.width`) to improve the readability of DataFrames when printed or displayed, ensuring comprehensive views of the data.\n",
    "*   **Execution Timestamp:** The start time of the notebook's execution is recorded and printed. This serves as a useful reference for tracking the duration of the operations performed.\n",
    "\n",
    "**Significance:** A correctly configured environment is paramount for the seamless execution of the notebook. This setup ensures that all dependencies are met, custom tools like the `DataAggregator` are accessible, and that data can be effectively manipulated and visualized, thereby preventing runtime errors and facilitating a smooth analytical workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c6eb4",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src directory to path to import modules\n",
    "# Assumes notebook is run from the 'notebooks' directory\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our custom modules\n",
    "from src.aggregator import DataAggregator\n",
    "# from src.utils import configure_plotting # Optional\n",
    "\n",
    "# Configure plotting (optional)\n",
    "# configure_plotting()\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Display pandas DataFrames nicely\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Display current time for reference\n",
    "print(f\"Notebook Execution Started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2963d47",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 2. Data Aggregator Instantiation\n",
    "\n",
    "**Purpose:** This step focuses on creating an instance of the `DataAggregator` class. This object is pivotal as it encapsulates the specialized logic required to load, merge, and process the historical time-series data collected by `01_data_collection.ipynb`.\n",
    "\n",
    "**Functionality of `DataAggregator`:**\n",
    "*   **Data Loading and Merging:** The primary role of the `DataAggregator` is to scan a specified directory (e.g., `../data/`) for raw data files (typically timestamped CSVs). It then loads these individual files and merges them into a single, comprehensive `pandas` DataFrame. This unified DataFrame contains all historical records, sorted chronologically.\n",
    "*   **Feature Computation:** Based on game release dates (which it attempts to parse and standardize), the aggregator calculates various features. These features are typically aggregated metrics over defined time windows, such as:\n",
    "    *   **Pre-release metrics:** Average player counts, social media engagement (Reddit, Twitter), search interest (Google Trends), and YouTube statistics in the period leading up to a game's launch.\n",
    "    *   **Post-launch outcomes:** Peak player counts, average viewership, and other engagement metrics within specified periods after the game's release (e.g., 7-day peak, 30-day average).\n",
    "*   **Configuration:** The `DataAggregator` is initialized with a `data_dir` parameter, which points to the location of the raw data files. It may also have internal configurations for file patterns (e.g., `steam_data_*.csv*`) and default aggregation windows.\n",
    "\n",
    "**Expected Outcome Upon Execution:**\n",
    "*   Successful instantiation of the `DataAggregator` object, ready to perform its data processing tasks.\n",
    "*   The code cell will typically show the creation of the `aggregator` variable, which holds the instance of the `DataAggregator` class.\n",
    "\n",
    "**Significance:** Instantiating the `DataAggregator` is a prerequisite for transforming the raw, event-level data into a structured format suitable for machine learning. It bridges the gap between raw data collection and feature engineering, providing the tools to derive meaningful insights and predictive variables from the historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59408aca",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Initialize the aggregator\n",
    "# Point it to the directory where the collector saved the raw data files\n",
    "aggregator = DataAggregator(data_dir=\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9380f2",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 3. Consolidation of Historical Raw Data\n",
    "\n",
    "**Purpose:** This crucial step involves invoking the `load_merged_data()` method of the `DataAggregator` instance. The primary objective is to consolidate all individual raw data files, previously generated and saved by the `01_data_collection.ipynb` notebook, into a single, unified `pandas` DataFrame.\n",
    "\n",
    "**Process Details:**\n",
    "*   **File Discovery:** The `DataAggregator` scans the specified `data_dir` (e.g., `../data/`) for files matching a predefined pattern (e.g., `steam_data_*.csv*` or `steam_data_*.csv.gz*`). This pattern ensures that all relevant historical data snapshots are identified.\n",
    "*   **Data Loading and Concatenation:** Each identified file is loaded into a `pandas` DataFrame. These individual DataFrames are then concatenated vertically to form one large DataFrame (`merged_df`).\n",
    "*   **Data Type Conversion and Sorting:** During or after loading, essential columns like `timestamp` and `release_date` are typically converted to appropriate datetime objects to facilitate time-based operations. The `merged_df` is usually sorted by `timestamp` to ensure chronological order, which is vital for time-series analysis and correct feature aggregation.\n",
    "*   **Error Handling:** The process includes try-except blocks to gracefully handle potential issues, such as no data files being found or errors during file loading. If no data is loaded, an empty DataFrame is typically returned, and a message is printed.\n",
    "\n",
    "**Expected Output Upon Successful Execution:**\n",
    "*   **`merged_df` DataFrame:** A `pandas` DataFrame named `merged_df` is created. This DataFrame contains all records from all previous data collection runs.\n",
    "    *   Each row represents a data snapshot for a specific game at a particular timestamp.\n",
    "    *   Columns include game identifiers (`app_id`, `name`), the `timestamp` of collection, all collected metrics (player counts, viewership, social media stats, etc.), and static game information like `release_date` and `metacritic_score` (if available).\n",
    "*   **Console Output:**\n",
    "    *   A sample of the `merged_df` (e.g., the first few rows via `display(merged_df.head())`).\n",
    "    *   The shape of `merged_df` (number of rows and columns).\n",
    "    *   The overall date range covered by the `timestamp` column in `merged_df`.\n",
    "*   **Error/Warning Messages:** If no data files are found or an error occurs, an appropriate message is printed to the console.\n",
    "\n",
    "**Significance:** The `merged_df` represents the complete raw historical dataset. It serves as the foundational input for the subsequent feature aggregation step, where time-series data will be transformed into a one-row-per-game feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa6c6d",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Load and merge all data files matching the default pattern 'steam_data_*.csv*'\n",
    "try:\n",
    "    merged_df = aggregator.load_merged_data()\n",
    "    if not merged_df.empty:\n",
    "        print(\"\\n--- Merged Raw Data Sample ---\")\n",
    "        display(merged_df.head())\n",
    "        print(f\"\\nShape of merged data: {merged_df.shape}\")\n",
    "        print(f\"Date range: {merged_df['timestamp'].min()} to {merged_df['timestamp'].max()}\")\n",
    "    else:\n",
    "        print(\"No raw data files found or loaded. Cannot proceed with aggregation.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading merged data: {e}\")\n",
    "    merged_df = pd.DataFrame() # Ensure df is empty on error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cfdfbf",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 4. Feature Aggregation from Time-Series Data\n",
    "\n",
    "**Purpose:** This is the core data transformation step where the longitudinal (time-series) `merged_df` is processed by the `DataAggregator`'s `aggregate_features` method. The goal is to convert the multi-entry per-game data into a single-row-per-game DataFrame, where columns represent engineered features and potential outcome variables.\n",
    "\n",
    "**Process Details:**\n",
    "*   **Input Data:** The primary input is the `merged_df` DataFrame, which contains all historical raw data.\n",
    "*   **Aggregation Logic:** The `aggregate_features` method iterates through each unique game identified in `merged_df`. For each game, it uses the game's `release_date` as a crucial reference point to define specific time windows for aggregation.\n",
    "*   **Configurable Time Windows:**\n",
    "    *   `PRE_RELEASE_DAYS` (e.g., 30 days): Defines the period *before* the game's release from which pre-launch hype indicators (e.g., average Google Trends score, Reddit activity, YouTube views/likes) are calculated. These serve as predictive features.\n",
    "    *   `POST_LAUNCH_PEAK_DAYS` (e.g., 7 days): Defines the initial period *after* launch used to determine peak performance metrics (e.g., peak Steam player count, peak Twitch viewers). These can be target variables or features.\n",
    "    *   `POST_LAUNCH_AVG_DAYS` (e.g., 30 days): Defines a broader period *after* launch for calculating average performance metrics (e.g., average player count, average viewership). These can also serve as target variables or features.\n",
    "*   **Feature Calculation:** Within these windows, various aggregation functions (mean, max, sum, etc.) are applied to the relevant metric columns (e.g., `player_count`, `twitch_viewer_count`, `reddit_subscribers`).\n",
    "*   **Output DataFrame (`aggregated_features_df`):**\n",
    "    *   Each row represents a unique game.\n",
    "    *   Columns include:\n",
    "        *   Static game information: `app_id`, `game_name`, `release_date`, `metacritic_score`.\n",
    "        *   Pre-release aggregated features: e.g., `google_trends_avg_pre_30d`, `reddit_posts_avg_pre_30d`.\n",
    "        *   Post-launch aggregated outcomes/features: e.g., `steam_peak_players_7d`, `twitch_avg_viewers_30d`.\n",
    "*   **Error Handling:** The process includes checks for the availability of `merged_df`. If `merged_df` is empty, aggregation is skipped. Errors during the aggregation process itself (e.g., issues with date parsing, insufficient data for a game within defined windows) are caught, and messages are printed.\n",
    "\n",
    "**Expected Output Upon Successful Execution:**\n",
    "*   **`aggregated_features_df` DataFrame:** A new `pandas` DataFrame where each game has a single row containing its static details and the calculated pre-release and post-launch aggregated metrics.\n",
    "*   **Console Output:**\n",
    "    *   A message indicating the start of feature aggregation.\n",
    "    *   A sample of the `aggregated_features_df` (e.g., `display(aggregated_features_df.head())`).\n",
    "    *   The shape of `aggregated_features_df`.\n",
    "    *   A list of all column names in `aggregated_features_df`.\n",
    "*   **Warning/Error Messages:** If aggregation results in an empty DataFrame (e.g., due to issues with release dates or insufficient data range for any game), a specific warning is printed. Other exceptions during aggregation are also reported.\n",
    "\n",
    "**Significance:** This step is pivotal as it transforms raw, granular time-series data into a structured, feature-rich dataset that is directly usable for training machine learning models. The engineered features capture temporal dynamics (pre-release hype vs. post-launch performance) crucial for predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a57050",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Aggregate features if merged data is available\n",
    "aggregated_features_df = pd.DataFrame() # Initialize empty\n",
    "if 'merged_df' in locals() and not merged_df.empty:\n",
    "    print(\"\\nStarting feature aggregation...\")\n",
    "    try:\n",
    "        # Define aggregation windows (can be adjusted)\n",
    "        PRE_RELEASE_DAYS = 30\n",
    "        POST_LAUNCH_PEAK_DAYS = 7\n",
    "        POST_LAUNCH_AVG_DAYS = 30\n",
    "\n",
    "        aggregated_features_df = aggregator.aggregate_features(\n",
    "            merged_data=merged_df,\n",
    "            pre_release_days=PRE_RELEASE_DAYS,\n",
    "            post_launch_days_peak=POST_LAUNCH_PEAK_DAYS,\n",
    "            post_launch_days_avg=POST_LAUNCH_AVG_DAYS\n",
    "        )\n",
    "\n",
    "        if not aggregated_features_df.empty:\n",
    "            print(\"\\n--- Aggregated Features Sample ---\")\n",
    "            display(aggregated_features_df.head())\n",
    "            print(f\"\\nShape of aggregated data: {aggregated_features_df.shape}\")\n",
    "            print(\"\\nColumns:\", aggregated_features_df.columns.tolist())\n",
    "        else:\n",
    "            print(\"Aggregation resulted in an empty DataFrame. Check data quality (e.g., release dates, sufficient time range).\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during feature aggregation: {e}\")\n",
    "else:\n",
    "    print(\"Skipping aggregation because merged data is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b4228",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 5. Preliminary Analysis and Cleaning of Aggregated Features\n",
    "\n",
    "**Purpose:** After feature aggregation, this section focuses on conducting an initial examination and basic cleaning of the `aggregated_features_df`. The goal is to understand the characteristics of the engineered feature set, identify potential data quality issues (like missing values), and perform rudimentary data preparation steps before more advanced modeling.\n",
    "\n",
    "**Process Details:**\n",
    "*   **Conditional Execution:** Analysis is performed only if `aggregated_features_df` was successfully created and is not empty.\n",
    "*   **1. Basic Information and Data Types (`.info()`):**\n",
    "    *   Provides a concise summary of the DataFrame, including the data type of each column, the number of non-null values, and memory usage.\n",
    "    *   Helps verify that columns have been assigned appropriate types (e.g., numerical features are numeric, dates are datetime objects).\n",
    "*   **2. Missing Value Analysis (`.isnull().sum()`):**\n",
    "    *   Calculates the percentage of missing values for each column.\n",
    "    *   Highlights columns with significant amounts of missing data, which might require imputation, removal, or indicate issues in data collection/aggregation for those features.\n",
    "    *   The code includes a commented-out example of a simple imputation strategy (filling numerical NaNs with 0), emphasizing that a more sophisticated approach might be needed.\n",
    "*   **3. Descriptive Statistics (`.describe()`):**\n",
    "    *   Generates summary statistics (count, mean, std, min, max, quartiles) for all numerical columns in `aggregated_features_df`.\n",
    "    *   Offers insights into the distribution, central tendency, and spread of each feature, helping to identify outliers or unusual data ranges.\n",
    "*   **4. Correlation Analysis (Partial, Focused on a Target Variable):**\n",
    "    *   Aims to understand the linear relationships between potential predictive features and a chosen target variable (e.g., `steam_peak_players_7d`).\n",
    "    *   Selects only numerical columns for correlation calculation.\n",
    "    *   Calculates the Pearson correlation coefficients of all numerical features with the specified target variable and sorts them.\n",
    "    *   A heatmap of the correlation matrix for all numerical features is generated to visualize inter-feature correlations and feature-target correlations more broadly.\n",
    "    *   This step helps in initial feature selection by identifying features that show a strong correlation (positive or negative) with the outcome of interest.\n",
    "\n",
    "**Expected Output Upon Successful Execution:**\n",
    "*   **Console Output:**\n",
    "    *   Basic info printout from `aggregated_features_df.info()`.\n",
    "    *   A list or Series showing columns with missing values and their respective percentages.\n",
    "    *   A DataFrame displaying descriptive statistics for numerical features.\n",
    "    *   A Series showing the correlation of numerical features with the defined `target_var`.\n",
    "*   **Visualizations:** A heatmap of the correlation matrix will be displayed.\n",
    "*   **Warning/Error Messages:** If `aggregated_features_df` is empty, a message indicating that the analysis is being skipped will be printed. Messages if the target variable for correlation is not found.\n",
    "\n",
    "**Significance:** This initial analysis is crucial for several reasons:\n",
    "*   **Data Quality Assessment:** It provides a first look at the quality and completeness of the engineered features.\n",
    "*   **Informing Preprocessing:** Identifies the need for missing value imputation, outlier handling, or feature scaling.\n",
    "*   **Preliminary Feature Relevance:** Correlation analysis offers early clues about which features might be most predictive for the chosen target, guiding subsequent feature selection and modeling efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6b4f5",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Analyze the aggregated features if available\n",
    "if 'aggregated_features_df' in locals() and not aggregated_features_df.empty:\n",
    "    print(\"\\n--- Initial Analysis of Aggregated Features ---\")\n",
    "\n",
    "    # 1. Basic Info and Data Types\n",
    "    print(\"\\nBasic Info:\")\n",
    "    aggregated_features_df.info()\n",
    "\n",
    "    # 2. Missing Value Analysis\n",
    "    print(\"\\nMissing Values (%):\")\n",
    "    missing_percent = (aggregated_features_df.isnull().sum() / len(aggregated_features_df)) * 100\n",
    "    print(missing_percent[missing_percent > 0].sort_values(ascending=False))\n",
    "\n",
    "    # Fill NaNs with 0 for numerical columns (excluding identifiers like app_id)     \n",
    "    numerical_cols = aggregated_features_df.select_dtypes(include=np.number).columns.tolist()\n",
    "    # Exclude identifiers like app_id\n",
    "    cols_to_fill = [col for col in numerical_cols if col not in ['app_id']]\n",
    "    print(f\"\\nFilling NaNs with 0 for columns: {cols_to_fill}\")\n",
    "    aggregated_features_df[cols_to_fill] = aggregated_features_df[cols_to_fill].fillna(0)\n",
    "    print(\"\\nMissing Values after filling with 0:\")\n",
    "    print(aggregated_features_df.isnull().sum()[aggregated_features_df.isnull().sum() > 0].sort_values(ascending=False))\n",
    "    # Note: A more sophisticated strategy (e.g., imputation) might be needed later.\n",
    "\n",
    "    # 3. Descriptive Statistics\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    display(aggregated_features_df.describe())\n",
    "\n",
    "    # 4. Correlation Analysis (Focus on potential features vs. target)\n",
    "    print(\"\\nCorrelation Matrix (Partial):\")\n",
    "    # Define potential target variable(s) using variable from previous cell\n",
    "    # Ensure POST_LAUNCH_PEAK_DAYS is defined (it should be from the aggregation cell)\n",
    "    if 'POST_LAUNCH_PEAK_DAYS' in locals():\n",
    "        target_var = f'steam_peak_players_{POST_LAUNCH_PEAK_DAYS}d'\n",
    "    else:\n",
    "        # Fallback if POST_LAUNCH_PEAK_DAYS is not defined, though it should be.\n",
    "        # This might indicate an issue with notebook execution order.\n",
    "        print(\"Warning: POST_LAUNCH_PEAK_DAYS not found, using default 7 for target_var.\")\n",
    "        target_var = 'steam_peak_players_7d' \n",
    "\n",
    "    if target_var in aggregated_features_df.columns:\n",
    "        # Select numerical columns for correlation\n",
    "        # Ensure all columns intended for correlation are numeric after imputation\n",
    "        corr_df = aggregated_features_df.select_dtypes(include=np.number)\n",
    "        \n",
    "        # Check if target_var is actually numeric and present in corr_df\n",
    "        if target_var in corr_df.columns and pd.api.types.is_numeric_dtype(corr_df[target_var]):\n",
    "            # Calculate correlation with the target variable\n",
    "            correlations = corr_df.corr()[target_var].sort_values(ascending=False)\n",
    "            print(f\"Correlations with '{target_var}':\")\n",
    "            print(correlations)\n",
    "\n",
    "            plt.figure(figsize=(15, 12)) # Adjusted size for better readability\n",
    "            sns.heatmap(corr_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "            plt.title('Correlation Matrix of Numerical Features (Post-Imputation)')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Target variable '{target_var}' is not numeric or not found in numerical columns after imputation. Skipping correlation plot.\")\n",
    "            # Optionally, print all columns of corr_df to debug\n",
    "            # print(\"Columns in corr_df for heatmap:\", corr_df.columns.tolist())\n",
    "            # Display a heatmap of all available numeric columns if target is problematic\n",
    "            if not corr_df.empty and len(corr_df.columns) > 1:\n",
    "                print(\"Displaying correlation matrix for all available numerical features:\")\n",
    "                plt.figure(figsize=(15, 12))\n",
    "                sns.heatmap(corr_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "                plt.title('Correlation Matrix of All Numerical Features (Post-Imputation)')\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(\"Not enough numerical data to plot a correlation matrix.\")\n",
    "    else:\n",
    "        print(f\"Target variable '{target_var}' not found in aggregated_features_df columns.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping analysis: Aggregated features DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202cb50c",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 6. Persisting the Aggregated Feature Set\n",
    "\n",
    "**Purpose:** This section is dedicated to saving the final `aggregated_features_df` DataFrame to a persistent storage format, typically a CSV file. This step ensures that the engineered features are readily available for the subsequent modeling phase.\n",
    "\n",
    "**Process Details:**\n",
    "*   **Conditional Execution:** The save operation is performed only if the `aggregated_features_df` exists and is not empty. This prevents errors from attempting to save a non-existent or empty DataFrame, which could occur if previous aggregation or cleaning steps failed.\n",
    "*   **File Path Definition:** A target file path is constructed using `os.path.join()`. The standard location for this output is within the project's `data` directory (e.g., `../data/`), and the file is typically named `aggregated_game_features.csv`.\n",
    "*   **Saving to CSV:** The `pandas` DataFrame's `to_csv()` method is used to serialize the data. \n",
    "    *   `index=False` is specified to prevent `pandas` from writing the DataFrame index as a column in the CSV file, which is generally preferred for cleaner data loading in subsequent steps.\n",
    "*   **Error Handling:** A try-except block is implemented to catch and report any potential `IOError` or other exceptions that might occur during the file writing process (e.g., disk full, insufficient permissions).\n",
    "\n",
    "**Expected Output Upon Successful Execution:**\n",
    "*   **Console Output:**\n",
    "    *   A confirmation message indicating that the `aggregated_features_df` has been successfully saved, along with the full path to the output CSV file.\n",
    "*   **File System:** A new CSV file (e.g., `aggregated_game_features.csv`) will be created or overwritten in the specified `data` directory.\n",
    "*   **Warning/Error Messages:** If `aggregated_features_df` is empty, a message stating that the save operation is being skipped will be printed. If an error occurs during the save process, an error message detailing the issue will be displayed.\n",
    "\n",
    "**Significance:** Saving the aggregated features is a critical checkpoint. This persisted dataset forms the direct input for the `03_modeling.ipynb` notebook, where machine learning models will be trained and evaluated. It decouples the feature engineering process from the modeling process, allowing for modularity and easier iteration on either part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c22f104",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Save the aggregated features DataFrame\n",
    "if 'aggregated_features_df' in locals() and not aggregated_features_df.empty:\n",
    "    save_path = os.path.join(\"..\", \"data\", \"aggregated_game_features.csv\")\n",
    "    try:\n",
    "        aggregated_features_df.to_csv(save_path, index=False)\n",
    "        print(f\"\\nAggregated features saved successfully to: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving aggregated features: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping save: Aggregated features DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d5a88d",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 7. Conclusion and Next Steps: Transition to Predictive Modeling\n",
    "\n",
    "**Purpose:** This section concludes the feature engineering phase and outlines the critical next steps in the project, primarily focusing on the transition to the predictive modeling stage.\n",
    "\n",
    "**Summary of Achievements in this Notebook:**\n",
    "*   Successfully loaded and merged raw, time-stamped data collected by `01_data_collection.ipynb`.\n",
    "*   Utilized the `DataAggregator` to engineer a comprehensive set of features, distinguishing between pre-release indicators and post-launch performance metrics for each game.\n",
    "*   Conducted preliminary analysis, including missing value assessment (with a simple imputation strategy) and correlation analysis, to understand the characteristics of the aggregated feature set.\n",
    "*   Persisted the final `aggregated_game_features.df` to a CSV file, making it ready for input into the modeling phase.\n",
    "\n",
    "**Path Forward: `03_modeling.ipynb`**\n",
    "\n",
    "The primary next step is to proceed to the `03_modeling.ipynb` notebook. This notebook will leverage the `aggregated_game_features.csv` file generated here to develop and evaluate predictive models. Key activities in the modeling phase will include:\n",
    "\n",
    "1.  **Advanced Data Preprocessing & Feature Refinement:**\n",
    "    *   **Sophisticated Imputation:** Implement more advanced techniques for handling any remaining missing values (e.g., KNN imputation, model-based imputation) if the simple fill-with-zero approach proves insufficient.\n",
    "    *   **Feature Scaling/Normalization:** Apply appropriate scaling techniques (e.g., StandardScaler, MinMaxScaler) to numerical features to ensure they are on a comparable scale, which is often beneficial for many machine learning algorithms.\n",
    "    *   **Outlier Detection and Handling:** Investigate and address potential outliers in the feature set that might disproportionately influence model training.\n",
    "    *   **Categorical Feature Encoding:** Convert any categorical features (if present and relevant) into a numerical format suitable for modeling (e.g., one-hot encoding, label encoding).\n",
    "    *   **Feature Selection/Dimensionality Reduction:** Employ techniques (e.g., RFE, PCA, feature importance from tree-based models) to select the most relevant features or reduce dimensionality, potentially improving model performance and interpretability.\n",
    "\n",
    "2.  **Model Selection and Training:**\n",
    "    *   **Algorithm Exploration:** Experiment with a variety of machine learning algorithms suitable for regression tasks (predicting numerical outcomes like peak player counts). This could include linear models (e.g., Linear Regression, Ridge, Lasso), tree-based models (e.g., Decision Trees, Random Forests, Gradient Boosting Machines like XGBoost, LightGBM), and potentially neural networks.\n",
    "    *   **Train-Test Split:** Divide the dataset into training and testing sets to evaluate model generalization on unseen data.\n",
    "    *   **Cross-Validation:** Utilize cross-validation techniques during training to obtain robust performance estimates and mitigate overfitting.\n",
    "\n",
    "3.  **Hyperparameter Tuning:**\n",
    "    *   Optimize the hyperparameters of the chosen models using techniques like GridSearchCV or RandomizedSearchCV to maximize their predictive performance.\n",
    "\n",
    "4.  **Model Evaluation:**\n",
    "    *   Assess model performance using appropriate regression metrics (e.g., Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared).\n",
    "    *   Analyze prediction errors and identify areas where the model performs well or poorly.\n",
    "\n",
    "5.  **Interpretation and Iteration:**\n",
    "    *   Interpret the results, understand feature importances, and draw conclusions about the factors driving game popularity.\n",
    "    *   Iterate on the feature engineering and modeling process based on insights gained, potentially revisiting this notebook (`02_feature_engineering.ipynb`) to create new features or refine existing ones.\n",
    "\n",
    "**Significance:** The successful completion of this feature engineering notebook provides a clean, structured, and feature-rich dataset. This dataset is the cornerstone upon which the predictive models will be built, directly impacting the potential accuracy and insights derived from the final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae6896",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Final summary message\n",
    "print(\"\\nFeature Engineering & Aggregation Notebook Complete.\")\n",
    "if 'save_path' in locals() and os.path.exists(save_path):\n",
    "    print(f\"Aggregated features ready for modeling at: {save_path}\")\n",
    "else:\n",
    "    print(\"Aggregated features were not saved (likely due to empty data or an error). Check previous cell outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "---\n",
    "*End of Notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
