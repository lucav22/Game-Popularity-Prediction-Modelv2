{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# 03. Predictive Modeling\n",
        "\n",
        "**Purpose:** This notebook focuses on building and evaluating machine learning models to predict game popularity based on the aggregated features created in the previous notebook (`02_feature_engineering.ipynb`).\n",
        "\n",
        "**Why This Matters:** This is the core prediction step where we leverage the collected and processed data to train models that can estimate a game's potential success (e.g., peak player count) before or shortly after launch.\n",
        "\n",
        "**What to Expect:** After running this notebook, you will:\n",
        "1. Load the aggregated feature dataset.\n",
        "2. Prepare the data for modeling (feature selection, splitting, scaling).\n",
        "3. Train several common regression models (e.g., Linear Regression, Random Forest, Gradient Boosting).\n",
        "4. Evaluate the models using standard regression metrics (R², MAE, RMSE).\n",
        "5. Compare model performance to identify the most promising approach.\n",
        "6. Potentially save the best-performing model for future use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "**Purpose:** Import necessary libraries for data manipulation, modeling, and evaluation.\n",
        "\n",
        "**Why:** Provides the tools needed for the machine learning workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Imports and Setup\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib # For saving models\n",
        "\n",
        "# Add src directory to path (optional, if utility functions are needed)\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "# from src.utils import configure_plotting # Optional\n",
        "\n",
        "# Configure plotting (optional)\n",
        "# configure_plotting()\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Display pandas DataFrames nicely\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# Display current time for reference\n",
        "print(f\"Notebook Execution Started: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 2. Load Aggregated Data\n",
        "\n",
        "**Purpose:** Load the feature set created by `02_feature_engineering.ipynb`.\n",
        "\n",
        "**Why:** This dataset contains the features and target variables needed for training the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Load the aggregated features dataset\n",
        "data_path = os.path.join(\"..\", \"data\", \"aggregated_game_features.csv\")\n",
        "try:\n",
        "    df_agg = pd.read_csv(data_path)\n",
        "    print(f\"Successfully loaded aggregated data from: {data_path}\")\n",
        "    print(f\"Shape: {df_agg.shape}\")\n",
        "    display(df_agg.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Aggregated data file not found at {data_path}\")\n",
        "    print(\"Please ensure '02_feature_engineering.ipynb' has been run successfully.\")\n",
        "    df_agg = pd.DataFrame() # Assign empty df to prevent errors later\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred loading the data: {e}\")\n",
        "    df_agg = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 3. Data Preparation for Modeling\n",
        "\n",
        "**Purpose:** Select features and target, handle missing values definitively, split data, and apply necessary transformations (e.g., scaling).\n",
        "\n",
        "**Why:** Machine learning models require clean, numerical input and separate training/testing sets for reliable evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Proceed only if data was loaded successfully\n",
        "if not df_agg.empty:\n",
        "    print(\"Preparing data for modeling...\")\n",
        "\n",
        "    # --- Define Features (X) and Target (y) ---\n",
        "    # Example: Predict peak players in the first 7 days\n",
        "    # Find the actual column name (it includes the number of days)\n",
        "    peak_days_col = next((col for col in df_agg.columns if col.startswith('steam_peak_players_')), None)\n",
        "    if peak_days_col is None:\n",
        "        raise ValueError(\"Target column 'steam_peak_players_*d' not found. Check aggregation notebook.\")\n",
        "    \n",
        "    target = peak_days_col\n",
        "    print(f\"Target variable (y): {target}\")\n",
        "\n",
        "    # Select potential features (pre-release metrics + static info)\n",
        "    # Exclude post-launch outcomes (except the target), identifiers, and dates\n",
        "    potential_features = [\n",
        "        'metacritic_score',\n",
        "        'google_trends_avg_pre',\n",
        "        'reddit_posts_avg_pre',\n",
        "        'twitter_count_avg_pre',\n",
        "        'reddit_subs_pre',\n",
        "        'reddit_active_pre',\n",
        "        # Add YouTube pre-release features if aggregated\n",
        "        # 'youtube_total_views_pre', # Example if added in aggregator\n",
        "        # 'youtube_avg_likes_pre',   # Example if added in aggregator\n",
        "        # Potentially add genre, price etc. if properly encoded\n",
        "    ]\n",
        "    \n",
        "    # Filter to only include features present in the DataFrame\n",
        "    features = [f for f in potential_features if f in df_agg.columns]\n",
        "    print(f\"Selected features (X): {features}\")\n",
        "\n",
        "    # --- Handle Missing Values ---\n",
        "    # Drop rows where the target variable is missing\n",
        "    df_model = df_agg.dropna(subset=[target]).copy()\n",
        "    print(f\"Shape after dropping rows with missing target: {df_model.shape}\")\n",
        "\n",
        "    # Impute missing values in features (using median for simplicity)\n",
        "    for col in features:\n",
        "        if df_model[col].isnull().any():\n",
        "            median_val = df_model[col].median()\n",
        "            df_model[col].fillna(median_val, inplace=True)\n",
        "            print(f\"Imputed missing values in '{col}' with median ({median_val:.2f})\")\n",
        "            \n",
        "    # Verify no missing values remain in features or target\n",
        "    print(f\"Missing values in target: {df_model[target].isnull().sum()}\")\n",
        "    print(f\"Missing values in features: {df_model[features].isnull().sum().sum()}\")\n",
        "\n",
        "    # --- Split Data ---\n",
        "    X = df_model[features]\n",
        "    y = df_model[target]\n",
        "    \n",
        "    if len(df_model) < 10:\n",
        "         print(\"Warning: Very small dataset, results may not be reliable. Consider collecting more data.\")\n",
        "         # Handle small dataset case if necessary, e.g., skip splitting or use cross-validation\n",
        "         X_train, X_test, y_train, y_test = X, X, y, y # Use all data for train/test - not ideal!\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        print(f\"Training set shape: X={X_train.shape}, y={y_train.shape}\")\n",
        "        print(f\"Testing set shape: X={X_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "    # --- Preprocessing (Scaling) ---\n",
        "    # Scale numerical features\n",
        "    # Note: If categorical features were added, OneHotEncoder would be included here\n",
        "    scaler = StandardScaler()\n",
        "    \n",
        "    # Fit scaler on training data only, then transform both train and test\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Convert scaled arrays back to DataFrames for easier inspection (optional)\n",
        "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=features, index=X_train.index)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=features, index=X_test.index)\n",
        "    print(\"\\nNumerical features scaled using StandardScaler.\")\n",
        "    # display(X_train_scaled.head())\n",
        "else:\n",
        "    print(\"Skipping data preparation as aggregated data is empty.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 4. Model Training and Evaluation\n",
        "\n",
        "**Purpose:** Train different regression models and evaluate their performance on the test set.\n",
        "\n",
        "**Why:** To compare how well different algorithms can predict the target variable using the prepared features.\n",
        "\n",
        "**Expected Output:** Performance metrics (R², MAE, RMSE) for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Proceed only if data preparation was successful\n",
        "model_results = {}\n",
        "if 'X_train_scaled' in locals():\n",
        "    print(\"\\n--- Training and Evaluating Models ---\")\n",
        "    \n",
        "    # Define models to try\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "    }\n",
        "    \n",
        "    # Loop through models, train, predict, and evaluate\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        # Make predictions on the test set\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        \n",
        "        # Evaluate the model\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        \n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - start_time\n",
        "        \n",
        "        # Store results\n",
        "        model_results[name] = {\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'R2': r2,\n",
        "            'Training Time': duration\n",
        "        }\n",
        "        \n",
        "        print(f\"  MAE: {mae:.2f}\")\n",
        "        print(f\"  RMSE: {rmse:.2f}\")\n",
        "        print(f\"  R2 Score: {r2:.3f}\")\n",
        "        print(f\"  Training Time: {duration}\")\n",
        "        \n",
        "        # Optional: Feature importance for tree-based models\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n",
        "            print(\"\\n  Feature Importances:\")\n",
        "            print(importances.head(10)) # Display top 10\n",
        "            # plt.figure(figsize=(10, 6))\n",
        "            # importances.plot(kind='bar')\n",
        "            # plt.title(f'{name} Feature Importances')\n",
        "            # plt.ylabel('Importance')\n",
        "            # plt.show()\n",
        "            \n",
        "else:\n",
        "    print(\"Skipping model training as data preparation failed or was skipped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 5. Model Comparison\n",
        "\n",
        "**Purpose:** Compare the performance metrics of the different models.\n",
        "\n",
        "**Why:** To identify the best-performing model based on the chosen evaluation criteria (e.g., lowest RMSE, highest R²).\n",
        "\n",
        "**Expected Output:** A summary table or visualization comparing model metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Compare model results if available\n",
        "if model_results:\n",
        "    results_df = pd.DataFrame.from_dict(model_results, orient='index')\n",
        "    results_df = results_df.sort_values('RMSE', ascending=True) # Sort by RMSE (lower is better)\n",
        "    \n",
        "    print(\"\\n--- Model Performance Comparison ---\")\n",
        "    display(results_df)\n",
        "    \n",
        "    # Identify the best model based on RMSE\n",
        "    best_model_name = results_df.index[0]\n",
        "    print(f\"\\nBest model based on RMSE: {best_model_name}\")\n",
        "    \n",
        "    # Optional: Plot comparison\n",
        "    # results_df[['MAE', 'RMSE']].plot(kind='bar', figsize=(10, 6))\n",
        "    # plt.title('Model Error Comparison (Lower is Better)')\n",
        "    # plt.ylabel('Error')\n",
        "    # plt.xticks(rotation=0)\n",
        "    # plt.show()\n",
        "    # \n",
        "    # results_df['R2'].plot(kind='bar', figsize=(10, 6))\n",
        "    # plt.title('Model R2 Score Comparison (Higher is Better)')\n",
        "    # plt.ylabel('R2 Score')\n",
        "    # plt.ylim(min(0, results_df['R2'].min() - 0.1), 1.0)\n",
        "    # plt.xticks(rotation=0)\n",
        "    # plt.show()\n",
        "else:\n",
        "    print(\"Skipping model comparison as no models were trained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 6. Save Best Model (Optional)\n",
        "\n",
        "**Purpose:** Persist the trained model for future use (e.g., deployment or making predictions on new data).\n",
        "\n",
        "**Why:** Avoids retraining the model every time predictions are needed.\n",
        "\n",
        "**Expected Output:** Confirmation that the model and scaler have been saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Save the best performing model and the scaler\n",
        "if model_results and 'best_model_name' in locals():\n",
        "    best_model = models[best_model_name] # Get the trained model instance\n",
        "    \n",
        "    # Create directory if it doesn't exist\n",
        "    model_dir = os.path.join(\"..\", \"models\")\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    # Define file paths\n",
        "    model_path = os.path.join(model_dir, f\"{best_model_name.lower().replace(' ', '_')}_model.joblib\")\n",
        "    scaler_path = os.path.join(model_dir, \"scaler.joblib\")\n",
        "    features_path = os.path.join(model_dir, \"features.joblib\")\n",
        "    \n",
        "    try:\n",
        "        # Save the model\n",
        "        joblib.dump(best_model, model_path)\n",
        "        print(f\"\\nBest model ({best_model_name}) saved to: {model_path}\")\n",
        "        \n",
        "        # Save the scaler\n",
        "        joblib.dump(scaler, scaler_path)\n",
        "        print(f\"Scaler saved to: {scaler_path}\")\n",
        "        \n",
        "        # Save the list of features used by the model\n",
        "        joblib.dump(features, features_path)\n",
        "        print(f\"Feature list saved to: {features_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving model/scaler/features: {e}\")\n",
        "else:\n",
        "    print(\"\\nSkipping model saving: No best model identified or training failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 7. Conclusion and Next Steps\n",
        "\n",
        "**Purpose:** Summarize the modeling results and suggest future directions.\n",
        "\n",
        "**Why:** Provides closure to the modeling process and identifies areas for improvement.\n",
        "\n",
        "**Next Actions:**\n",
        "1.  **Hyperparameter Tuning:** Fine-tune the best model's parameters for potentially better performance.\n",
        "2.  **Advanced Feature Engineering:** Create more sophisticated features (e.g., interaction terms, time-based features if more data is available).\n",
        "3.  **Try Other Models:** Experiment with different algorithms (e.g., XGBoost, LightGBM, SVM).\n",
        "4.  **Deployment:** If performance is satisfactory, consider deploying the model to make predictions on new, unseen games.\n",
        "5.  **Collect More Data:** A larger dataset, especially with more games and longer historical data, will likely improve model robustness and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Final summary message\n",
        "print(\"\\nPredictive Modeling Notebook Complete.\")\n",
        "if model_results:\n",
        "    print(f\"Model comparison complete. Best model based on RMSE: {best_model_name}\")\n",
        "    if 'model_path' in locals() and os.path.exists(model_path):\n",
        "        print(f\"Best model artifacts saved in: {model_dir}\")\n",
        "else:\n",
        "    print(\"Model training and evaluation were skipped or failed. Check previous cell outputs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "---\n",
        "*End of Notebook*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
