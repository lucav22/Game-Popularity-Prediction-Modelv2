{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# 03. Predictive Modeling\n",
        "\n",
        "**Purpose:** This notebook focuses on building and evaluating machine learning models to predict game popularity based on the aggregated features created in the previous notebook (`02_feature_engineering.ipynb`).\n",
        "\n",
        "**Why This Matters:** This is the core prediction step where we leverage the collected and processed data to train models that can estimate a game's potential success (e.g., peak player count) before or shortly after launch.\n",
        "\n",
        "**What to Expect:** After running this notebook, you will:\n",
        "1. Load the aggregated feature dataset.\n",
        "2. Prepare the data for modeling (feature selection, splitting, scaling).\n",
        "3. Train several common regression models (e.g., Linear Regression, Random Forest, Gradient Boosting).\n",
        "4. Evaluate the models using standard regression metrics (R², MAE, RMSE).\n",
        "5. Compare model performance to identify the most promising approach.\n",
        "6. Potentially save the best-performing model for future use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "**Purpose:** Import necessary libraries for data manipulation, modeling, and evaluation.\n",
        "\n",
        "**Why:** Provides the tools needed for the machine learning workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Notebook Execution Started: 2025-05-06 22:23:29.199573\n"
          ]
        }
      ],
      "source": [
        "# Imports and Setup\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib # For saving models\n",
        "\n",
        "# Add src directory to path (optional, if utility functions are needed)\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "# from src.utils import configure_plotting # Optional\n",
        "\n",
        "# Configure plotting (optional)\n",
        "# configure_plotting()\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Display pandas DataFrames nicely\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# Display current time for reference\n",
        "print(f\"Notebook Execution Started: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 2. Load Aggregated Data\n",
        "\n",
        "**Purpose:** Load the feature set created by `02_feature_engineering.ipynb`.\n",
        "\n",
        "**Why:** This dataset contains the features and target variables needed for training the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded aggregated data from: ..\\data\\aggregated_game_features.csv\n",
            "Shape: (18, 13)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>app_id</th>\n",
              "      <th>game_name</th>\n",
              "      <th>release_date</th>\n",
              "      <th>metacritic_score</th>\n",
              "      <th>google_trends_avg_pre</th>\n",
              "      <th>reddit_posts_avg_pre</th>\n",
              "      <th>twitter_count_avg_pre</th>\n",
              "      <th>reddit_subs_pre</th>\n",
              "      <th>reddit_active_pre</th>\n",
              "      <th>steam_peak_players_7d</th>\n",
              "      <th>twitch_peak_viewers_7d</th>\n",
              "      <th>steam_avg_players_30d</th>\n",
              "      <th>twitch_avg_viewers_30d</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>440</td>\n",
              "      <td>Team Fortress 2</td>\n",
              "      <td>2007-10-10</td>\n",
              "      <td>92.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570</td>\n",
              "      <td>Dota 2</td>\n",
              "      <td>2013-07-09</td>\n",
              "      <td>90.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>730</td>\n",
              "      <td>Counter-Strike 2</td>\n",
              "      <td>2012-08-21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>233860</td>\n",
              "      <td>Kenshi</td>\n",
              "      <td>2018-12-06</td>\n",
              "      <td>75.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>252950</td>\n",
              "      <td>Rocket League®</td>\n",
              "      <td>2015-07-06</td>\n",
              "      <td>86.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   app_id         game_name release_date  metacritic_score  google_trends_avg_pre  reddit_posts_avg_pre  twitter_count_avg_pre  reddit_subs_pre  reddit_active_pre  steam_peak_players_7d  twitch_peak_viewers_7d  steam_avg_players_30d  twitch_avg_viewers_30d\n",
              "0     440   Team Fortress 2   2007-10-10              92.0                    NaN                   NaN                    NaN              NaN                NaN                    NaN                     NaN                    NaN                     NaN\n",
              "1     570            Dota 2   2013-07-09              90.0                    NaN                   NaN                    NaN              NaN                NaN                    NaN                     NaN                    NaN                     NaN\n",
              "2     730  Counter-Strike 2   2012-08-21               NaN                    NaN                   NaN                    NaN              NaN                NaN                    NaN                     NaN                    NaN                     NaN\n",
              "3  233860            Kenshi   2018-12-06              75.0                    NaN                   NaN                    NaN              NaN                NaN                    NaN                     NaN                    NaN                     NaN\n",
              "4  252950    Rocket League®   2015-07-06              86.0                    NaN                   NaN                    NaN              NaN                NaN                    NaN                     NaN                    NaN                     NaN"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the aggregated features dataset\n",
        "data_path = os.path.join(\"..\", \"data\", \"aggregated_game_features.csv\")\n",
        "try:\n",
        "    df_agg = pd.read_csv(data_path)\n",
        "    print(f\"Successfully loaded aggregated data from: {data_path}\")\n",
        "    print(f\"Shape: {df_agg.shape}\")\n",
        "    display(df_agg.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Aggregated data file not found at {data_path}\")\n",
        "    print(\"Please ensure '02_feature_engineering.ipynb' has been run successfully.\")\n",
        "    df_agg = pd.DataFrame() # Assign empty df to prevent errors later\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred loading the data: {e}\")\n",
        "    df_agg = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 3. Data Preparation for Modeling\n",
        "\n",
        "**Purpose:** Select features and target, handle missing values definitively, split data, and apply necessary transformations (e.g., scaling).\n",
        "\n",
        "**Why:** Machine learning models require clean, numerical input and separate training/testing sets for reliable evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing data for modeling...\n",
            "Target variable (y): steam_peak_players_7d\n",
            "Selected features (X): ['metacritic_score', 'google_trends_avg_pre', 'reddit_posts_avg_pre', 'twitter_count_avg_pre', 'reddit_subs_pre', 'reddit_active_pre']\n",
            "Shape after dropping rows with missing target: (0, 13)\n",
            "Missing values in target: 0\n",
            "Missing values in features: 0\n",
            "Warning: Very small dataset, results may not be reliable. Consider collecting more data.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by StandardScaler.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     66\u001b[39m scaler = StandardScaler()\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Fit scaler on training data only, then transform both train and test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m X_train_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m X_test_scaled = scaler.transform(X_test)\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Convert scaled arrays back to DataFrames for easier inspection (optional)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucav\\Github\\Game-Popularity-Prediction-Modelv2\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucav\\Github\\Game-Popularity-Prediction-Modelv2\\venv\\Lib\\site-packages\\sklearn\\base.py:918\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    903\u001b[39m         warnings.warn(\n\u001b[32m    904\u001b[39m             (\n\u001b[32m    905\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    914\u001b[39m         )\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucav\\Github\\Game-Popularity-Prediction-Modelv2\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucav\\Github\\Game-Popularity-Prediction-Modelv2\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucav\\Github\\Game-Popularity-Prediction-Modelv2\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:930\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    899\u001b[39m \n\u001b[32m    900\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    927\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    929\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucav\\Github\\Game-Popularity-Prediction-Modelv2\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucav\\Github\\Game-Popularity-Prediction-Modelv2\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1128\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1133\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1134\u001b[39m         )\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1137\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
            "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by StandardScaler."
          ]
        }
      ],
      "source": [
        "# Proceed only if data was loaded successfully\n",
        "if not df_agg.empty:\n",
        "    print(\"Preparing data for modeling...\")\n",
        "\n",
        "    # --- Define Features (X) and Target (y) ---\n",
        "    # Example: Predict peak players in the first 7 days\n",
        "    # Find the actual column name (it includes the number of days)\n",
        "    peak_days_col = next((col for col in df_agg.columns if col.startswith('steam_peak_players_')), None)\n",
        "    if peak_days_col is None:\n",
        "        raise ValueError(\"Target column 'steam_peak_players_*d' not found. Check aggregation notebook.\")\n",
        "    \n",
        "    target = peak_days_col\n",
        "    print(f\"Target variable (y): {target}\")\n",
        "\n",
        "    # Select potential features (pre-release metrics + static info)\n",
        "    # Exclude post-launch outcomes (except the target), identifiers, and dates\n",
        "    potential_features = [\n",
        "        'metacritic_score',\n",
        "        'google_trends_avg_pre',\n",
        "        'reddit_posts_avg_pre',\n",
        "        'twitter_count_avg_pre',\n",
        "        'reddit_subs_pre',\n",
        "        'reddit_active_pre',\n",
        "        # Add YouTube pre-release features if aggregated\n",
        "        # 'youtube_total_views_pre', # Example if added in aggregator\n",
        "        # 'youtube_avg_likes_pre',   # Example if added in aggregator\n",
        "        # Potentially add genre, price etc. if properly encoded\n",
        "    ]\n",
        "    \n",
        "    # Filter to only include features present in the DataFrame\n",
        "    features = [f for f in potential_features if f in df_agg.columns]\n",
        "    print(f\"Selected features (X): {features}\")\n",
        "\n",
        "    # --- Handle Missing Values ---\n",
        "    # Drop rows where the target variable is missing\n",
        "    df_model = df_agg.dropna(subset=[target]).copy()\n",
        "    print(f\"Shape after dropping rows with missing target: {df_model.shape}\")\n",
        "\n",
        "    # Impute missing values in features (using median for simplicity)\n",
        "    for col in features:\n",
        "        if df_model[col].isnull().any():\n",
        "            median_val = df_model[col].median()\n",
        "            df_model[col].fillna(median_val, inplace=True)\n",
        "            print(f\"Imputed missing values in '{col}' with median ({median_val:.2f})\")\n",
        "            \n",
        "    # Verify no missing values remain in features or target\n",
        "    print(f\"Missing values in target: {df_model[target].isnull().sum()}\")\n",
        "    print(f\"Missing values in features: {df_model[features].isnull().sum().sum()}\")\n",
        "\n",
        "    # --- Split Data ---\n",
        "    X = df_model[features]\n",
        "    y = df_model[target]\n",
        "    \n",
        "    if len(df_model) < 10:\n",
        "         print(\"Warning: Very small dataset, results may not be reliable. Consider collecting more data.\")\n",
        "         # Handle small dataset case if necessary, e.g., skip splitting or use cross-validation\n",
        "         X_train, X_test, y_train, y_test = X, X, y, y # Use all data for train/test - not ideal!\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        print(f\"Training set shape: X={X_train.shape}, y={y_train.shape}\")\n",
        "        print(f\"Testing set shape: X={X_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "    # --- Preprocessing (Scaling) ---\n",
        "    # Scale numerical features\n",
        "    # Note: If categorical features were added, OneHotEncoder would be included here\n",
        "    scaler = StandardScaler()\n",
        "    \n",
        "    # Fit scaler on training data only, then transform both train and test\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Convert scaled arrays back to DataFrames for easier inspection (optional)\n",
        "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=features, index=X_train.index)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=features, index=X_test.index)\n",
        "    print(\"\\nNumerical features scaled using StandardScaler.\")\n",
        "    # display(X_train_scaled.head())\n",
        "else:\n",
        "    print(\"Skipping data preparation as aggregated data is empty.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 4. Model Training and Evaluation\n",
        "\n",
        "**Purpose:** Train different regression models and evaluate their performance on the test set.\n",
        "\n",
        "**Why:** To compare how well different algorithms can predict the target variable using the prepared features.\n",
        "\n",
        "**Expected Output:** Performance metrics (R², MAE, RMSE) for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Proceed only if data preparation was successful\n",
        "model_results = {}\n",
        "if 'X_train_scaled' in locals():\n",
        "    print(\"\\n--- Training and Evaluating Models ---\")\n",
        "    \n",
        "    # Define models to try\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "    }\n",
        "    \n",
        "    # Loop through models, train, predict, and evaluate\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        # Make predictions on the test set\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        \n",
        "        # Evaluate the model\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        \n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - start_time\n",
        "        \n",
        "        # Store results\n",
        "        model_results[name] = {\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'R2': r2,\n",
        "            'Training Time': duration\n",
        "        }\n",
        "        \n",
        "        print(f\"  MAE: {mae:.2f}\")\n",
        "        print(f\"  RMSE: {rmse:.2f}\")\n",
        "        print(f\"  R2 Score: {r2:.3f}\")\n",
        "        print(f\"  Training Time: {duration}\")\n",
        "        \n",
        "        # Optional: Feature importance for tree-based models\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n",
        "            print(\"\\n  Feature Importances:\")\n",
        "            print(importances.head(10)) # Display top 10\n",
        "            # plt.figure(figsize=(10, 6))\n",
        "            # importances.plot(kind='bar')\n",
        "            # plt.title(f'{name} Feature Importances')\n",
        "            # plt.ylabel('Importance')\n",
        "            # plt.show()\n",
        "            \n",
        "else:\n",
        "    print(\"Skipping model training as data preparation failed or was skipped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 5. Model Comparison\n",
        "\n",
        "**Purpose:** Compare the performance metrics of the different models.\n",
        "\n",
        "**Why:** To identify the best-performing model based on the chosen evaluation criteria (e.g., lowest RMSE, highest R²).\n",
        "\n",
        "**Expected Output:** A summary table or visualization comparing model metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Compare model results if available\n",
        "if model_results:\n",
        "    results_df = pd.DataFrame.from_dict(model_results, orient='index')\n",
        "    results_df = results_df.sort_values('RMSE', ascending=True) # Sort by RMSE (lower is better)\n",
        "    \n",
        "    print(\"\\n--- Model Performance Comparison ---\")\n",
        "    display(results_df)\n",
        "    \n",
        "    # Identify the best model based on RMSE\n",
        "    best_model_name = results_df.index[0]\n",
        "    print(f\"\\nBest model based on RMSE: {best_model_name}\")\n",
        "    \n",
        "    # Optional: Plot comparison\n",
        "    # results_df[['MAE', 'RMSE']].plot(kind='bar', figsize=(10, 6))\n",
        "    # plt.title('Model Error Comparison (Lower is Better)')\n",
        "    # plt.ylabel('Error')\n",
        "    # plt.xticks(rotation=0)\n",
        "    # plt.show()\n",
        "    # \n",
        "    # results_df['R2'].plot(kind='bar', figsize=(10, 6))\n",
        "    # plt.title('Model R2 Score Comparison (Higher is Better)')\n",
        "    # plt.ylabel('R2 Score')\n",
        "    # plt.ylim(min(0, results_df['R2'].min() - 0.1), 1.0)\n",
        "    # plt.xticks(rotation=0)\n",
        "    # plt.show()\n",
        "else:\n",
        "    print(\"Skipping model comparison as no models were trained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 6. Save Best Model (Optional)\n",
        "\n",
        "**Purpose:** Persist the trained model for future use (e.g., deployment or making predictions on new data).\n",
        "\n",
        "**Why:** Avoids retraining the model every time predictions are needed.\n",
        "\n",
        "**Expected Output:** Confirmation that the model and scaler have been saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Save the best performing model and the scaler\n",
        "if model_results and 'best_model_name' in locals():\n",
        "    best_model = models[best_model_name] # Get the trained model instance\n",
        "    \n",
        "    # Create directory if it doesn't exist\n",
        "    model_dir = os.path.join(\"..\", \"models\")\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    # Define file paths\n",
        "    model_path = os.path.join(model_dir, f\"{best_model_name.lower().replace(' ', '_')}_model.joblib\")\n",
        "    scaler_path = os.path.join(model_dir, \"scaler.joblib\")\n",
        "    features_path = os.path.join(model_dir, \"features.joblib\")\n",
        "    \n",
        "    try:\n",
        "        # Save the model\n",
        "        joblib.dump(best_model, model_path)\n",
        "        print(f\"\\nBest model ({best_model_name}) saved to: {model_path}\")\n",
        "        \n",
        "        # Save the scaler\n",
        "        joblib.dump(scaler, scaler_path)\n",
        "        print(f\"Scaler saved to: {scaler_path}\")\n",
        "        \n",
        "        # Save the list of features used by the model\n",
        "        joblib.dump(features, features_path)\n",
        "        print(f\"Feature list saved to: {features_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving model/scaler/features: {e}\")\n",
        "else:\n",
        "    print(\"\\nSkipping model saving: No best model identified or training failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 7. Conclusion and Next Steps\n",
        "\n",
        "**Purpose:** Summarize the modeling results and suggest future directions.\n",
        "\n",
        "**Why:** Provides closure to the modeling process and identifies areas for improvement.\n",
        "\n",
        "**Next Actions:**\n",
        "1.  **Hyperparameter Tuning:** Fine-tune the best model's parameters for potentially better performance.\n",
        "2.  **Advanced Feature Engineering:** Create more sophisticated features (e.g., interaction terms, time-based features if more data is available).\n",
        "3.  **Try Other Models:** Experiment with different algorithms (e.g., XGBoost, LightGBM, SVM).\n",
        "4.  **Deployment:** If performance is satisfactory, consider deploying the model to make predictions on new, unseen games.\n",
        "5.  **Collect More Data:** A larger dataset, especially with more games and longer historical data, will likely improve model robustness and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Final summary message\n",
        "print(\"\\nPredictive Modeling Notebook Complete.\")\n",
        "if model_results:\n",
        "    print(f\"Model comparison complete. Best model based on RMSE: {best_model_name}\")\n",
        "    if 'model_path' in locals() and os.path.exists(model_path):\n",
        "        print(f\"Best model artifacts saved in: {model_dir}\")\n",
        "else:\n",
        "    print(\"Model training and evaluation were skipped or failed. Check previous cell outputs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "---\n",
        "*End of Notebook*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
