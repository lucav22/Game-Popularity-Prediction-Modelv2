{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86dd2a50",
        "language": "markdown"
      },
      "source": [
        "# 01. Data Collection\n",
        "\n",
        "**Purpose:** This notebook demonstrates how to collect real-time game data using our custom `DataCollector` class. It fetches data from Steam (player counts, details), Twitch (viewership), and external sources like Google Trends, Reddit, Twitter, and YouTube.\n",
        "\n",
        "**Why This Matters:** This automated collection process provides fresh, multi-faceted data crucial for building our time series dataset and ultimately, the game popularity prediction model.\n",
        "\n",
        "**What to Expect:** After running this notebook, you will:\n",
        "1. Successfully collect current data from multiple APIs.\n",
        "2. Save the combined data in a structured format (compressed CSV).\n",
        "3. Understand the enhanced data collection workflow.\n",
        "4. Have the first data point for building a historical dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae2eb668",
        "language": "markdown"
      },
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "**Purpose:** Import necessary libraries and configure the environment.\n",
        "\n",
        "**Why:** Ensures the `DataCollector` and its dependencies can be found and used correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d43842a4",
      "metadata": {
        "id": "56b64a94",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Imports and Setup\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Add src directory to path to import modules\n",
        "# Assumes notebook is run from the 'notebooks' directory\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "\n",
        "# Import our custom modules\n",
        "from src.data_collector import DataCollector\n",
        "# from src.utils import configure_plotting # Optional: if plotting is needed here\n",
        "\n",
        "# Configure plotting (optional)\n",
        "# configure_plotting()\n",
        "\n",
        "# Display pandas DataFrames nicely\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# Display current time for reference\n",
        "print(f\"Notebook Execution Started: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6729227d",
      "metadata": {
        "id": "34393ae7",
        "language": "markdown"
      },
      "source": [
        "## 2. Initialize Data Collector\n",
        "\n",
        "**Purpose:** Create an instance of the `DataCollector`.\n",
        "\n",
        "**Why:** The collector manages API connections, game lists, and data storage. It automatically loads API keys from the `.env` file in the project root.\n",
        "\n",
        "**Expected Output:** Confirmation of initialization and the number of games being tracked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99fb9604",
      "metadata": {
        "id": "16d376da",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Initialize the collector\n",
        "# It will use the default game lists defined within the class\n",
        "# and look for API keys in the .env file in the project root\n",
        "# Ensure the data_dir path is correct relative to the notebook location\n",
        "collector = DataCollector(data_dir=\"../data\")\n",
        "\n",
        "# Optionally, view the game IDs being tracked\n",
        "all_game_ids = collector.get_all_game_ids()\n",
        "print(f\"Tracking {len(all_game_ids)} unique game IDs across categories.\")\n",
        "# print(collector.game_categories) # Uncomment to see the full category list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4dd867",
      "metadata": {
        "id": "7fb97b85",
        "language": "markdown"
      },
      "source": [
        "## 3. Collect Current Data\n",
        "\n",
        "**Purpose:** Execute the main data collection process.\n",
        "\n",
        "**Why:** This fetches the latest data from all configured sources (Steam, Twitch, Google Trends, Reddit, Twitter, YouTube).\n",
        "\n",
        "**IMPORTANT:** Before running this step:\n",
        "1.  **Check API Keys/Credentials:** Ensure your API keys/secrets/tokens in the `.env` file (located in the project root `c:\\\\Users\\\\lucav\\\\Github\\\\Game-Popularity-Prediction-Modelv2`) are correct, especially for **Reddit** (Client ID, Secret, User Agent) and **YouTube** (API Key). Incorrect credentials often lead to `401` or `403` errors.\n",
        "2.  **Check API Quotas/Rate Limits:** APIs like YouTube, Google Trends, and Twitter have usage limits (quotas) and rate limits. If you run this frequently or with many games, you might hit these limits, resulting in `403` (Quota Exceeded) or `429` (Too Many Requests) errors. Check your API provider dashboards (e.g., Google Cloud Console for YouTube) if you encounter persistent errors.\n",
        "\n",
        "**Expected Output:** \n",
        "- Status messages indicating which data sources are being queried.\n",
        "- Potential warnings or errors if API keys are invalid, quotas are exceeded, or rate limits are hit.\n",
        "- A DataFrame containing the combined data (if collection is at least partially successful).\n",
        "- A summary of the collected data shape and columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf04aebd",
      "metadata": {
        "id": "17b0d869",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Collect data for all tracked games\n",
        "# include_details=True is needed to get game names for Twitch/External lookups\n",
        "# include_twitch=True fetches Twitch viewership\n",
        "# include_external=True fetches Google Trends, Reddit, Twitter, YouTube data\n",
        "print(\"Starting data collection...\")\n",
        "print(\"This may take a few minutes depending on the number of games and API responsiveness.\")\n",
        "try:\n",
        "    current_data_df = collector.collect_current_data(\n",
        "        include_details=True,\n",
        "        include_twitch=True,\n",
        "        include_external=True\n",
        "    )\n",
        "    print(\"\\n--- Collected Data Sample ---\")\n",
        "    # Display relevant columns, especially the newly added ones\n",
        "    display_cols = [\n",
        "        'app_id', 'name', 'category', 'timestamp', 'player_count', 'twitch_viewer_count',\n",
        "        'google_trends_avg', 'reddit_subscribers', 'reddit_active_users', 'reddit_recent_posts', 'twitter_recent_count',\n",
        "        'youtube_total_views', 'youtube_avg_views', 'youtube_avg_likes', 'release_date'\n",
        "    ]\n",
        "    display_cols_present = [col for col in display_cols if col in current_data_df.columns]\n",
        "    display(current_data_df[display_cols_present].head())\n",
        "    print(f\"\\nShape: {current_data_df.shape}\")\n",
        "    print(\"\\nColumns:\", current_data_df.columns.tolist())\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during data collection: {e}\")\n",
        "    # Optionally re-raise if debugging: raise e\n",
        "    current_data_df = pd.DataFrame() # Ensure df exists but is empty on error"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6002d98",
      "metadata": {
        "id": "75faf4a7",
        "language": "markdown"
      },
      "source": [
        "## 4. Save Data to File\n",
        "\n",
        "**Purpose:** Save the collected DataFrame for future use.\n",
        "\n",
        "**Why:** Persistent storage allows us to build a historical dataset over time, which is essential for the `DataAggregator` and model training.\n",
        "\n",
        "**Expected Output:** Confirmation of the file save location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8ac21c",
      "metadata": {
        "id": "eba635e7",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Save the collected data to a compressed CSV file\n",
        "try:\n",
        "    # Check if the DataFrame exists and is not empty\n",
        "    if 'current_data_df' in locals() and not current_data_df.empty:\n",
        "        saved_filepath = collector.save_data(data=current_data_df, compress=True)\n",
        "        print(f\"\\nData successfully saved to: {saved_filepath}\")\n",
        "        \n",
        "        # Optional: Verify loading back\n",
        "        # loaded_data = collector.load_data(saved_filepath)\n",
        "        # print(f\"Verification - Loaded {len(loaded_data)} rows from saved file\")\n",
        "    else:\n",
        "        print(\"\\nSkipping save: No data collected or collection failed.\")\n",
        "except Exception as e:\n",
        "     print(f\"\\nAn error occurred while saving data: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f2f524",
      "metadata": {
        "id": "2d4debca",
        "language": "markdown"
      },
      "source": [
        "## 5. Initial Data Review (Optional)\n",
        "\n",
        "**Purpose:** Perform a quick check on the collected data.\n",
        "\n",
        "**Why:** Helps spot any immediate issues like missing values in key columns or unexpected data ranges.\n",
        "\n",
        "**Expected Output:**\n",
        "- Basic statistics for key numerical columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d56e5bb",
      "metadata": {
        "id": "10686c60",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Display basic statistics for numerical columns if data was collected\n",
        "if 'current_data_df' in locals() and not current_data_df.empty:\n",
        "    print(\"\\nBasic Statistics for Key Metrics:\")\n",
        "    stats_cols = [\n",
        "        'player_count', 'twitch_viewer_count', 'google_trends_avg', \n",
        "        'reddit_subscribers', 'reddit_active_users', 'reddit_recent_posts', \n",
        "        'twitter_recent_count', 'youtube_total_views', 'youtube_avg_views', 'youtube_avg_likes'\n",
        "    ]\n",
        "    stats_cols_present = [col for col in stats_cols if col in current_data_df.columns]\n",
        "    display(current_data_df[stats_cols_present].describe())\n",
        "else:\n",
        "    print(\"\\nSkipping statistics: No data available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12d6e99f",
      "metadata": {
        "id": "d1376301",
        "language": "markdown"
      },
      "source": [
        "## 6. Next Steps\n",
        "\n",
        "**Purpose:** Outline the path forward.\n",
        "\n",
        "**Why:** Guides the project towards the goal of building the prediction model.\n",
        "\n",
        "**Next Actions:**\n",
        "1.  **Repeat Collection:** Run this notebook periodically (e.g., daily, weekly) to build up historical data.\n",
        "2.  **Aggregation:** Once sufficient historical data exists, use the `DataAggregator` (likely in `02_feature_engineering.ipynb`) to process the saved files into a feature set.\n",
        "3.  **Feature Engineering & Modeling:** Analyze the aggregated data and build predictive models (`02_feature_engineering.ipynb`, `03_modeling.ipynb`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f91691b0",
      "metadata": {
        "id": "639c9ddf",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Final summary message\n",
        "print(\"\\nData Collection Notebook Complete.\")\n",
        "if 'saved_filepath' in locals():\n",
        "    print(f\"Latest data saved to: {saved_filepath}\")\n",
        "elif 'current_data_df' in locals() and current_data_df.empty:\n",
        "     print(\"Data collection run finished, but resulted in empty data or an error occurred.\")\n",
        "else:\n",
        "     print(\"Data collection run finished, but data was not saved (likely due to an error). Check previous cell outputs.\")\n",
        "\n",
        "print(\"\\nRemember to run this notebook periodically to build your historical dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58ed549a",
      "metadata": {
        "id": "8432d70e",
        "language": "markdown"
      },
      "source": [
        "---\n",
        "*End of Notebook*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
