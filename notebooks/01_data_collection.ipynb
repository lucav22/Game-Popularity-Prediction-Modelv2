{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bee791",
   "metadata": {
    "id": "86dd2a50",
    "language": "markdown"
   },
   "source": [
    "# 01. Data Collection: Acquiring Real-Time Game Metrics\n",
    "\n",
    "**Project Context:** This notebook forms a critical component of a final project aimed at developing a game popularity prediction model. The model's accuracy relies heavily on a comprehensive and timely dataset.\n",
    "\n",
    "**Purpose of this Notebook:** This document outlines and executes the initial data acquisition phase. It utilizes a custom-developed `DataCollector` class to programmatically fetch real-time data pertaining to video games from diverse online sources. These sources include:\n",
    "*   **Steam:** Player counts and detailed game information.\n",
    "*   **Twitch:** Live viewership statistics.\n",
    "*   **External Platforms:** Social sentiment and engagement metrics from Google Trends, Reddit, Twitter, and YouTube.\n",
    "\n",
    "**Significance:** The automated collection process detailed herein is fundamental for constructing a robust time-series dataset. This dataset will capture dynamic changes in game popularity indicators, providing the empirical basis for subsequent feature engineering and predictive modeling.\n",
    "\n",
    "**Expected Outcomes:** Upon successful execution of this notebook, the following will be achieved:\n",
    "1.  **Comprehensive Data Retrieval:** Current data points will be systematically collected from all specified APIs and web sources.\n",
    "2.  **Structured Data Storage:** The aggregated data will be saved in a compressed CSV format, ensuring efficient storage and accessibility for later stages of the project.\n",
    "3.  **Workflow Demonstration:** The notebook will illustrate the functionality and efficacy of the `DataCollector` class within the broader data pipeline.\n",
    "4.  **Foundation for Historical Analysis:** This initial data collection run will produce the first entry in what will become a longitudinal dataset, essential for time-series analysis and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2983527",
   "metadata": {
    "id": "ae2eb668",
    "language": "markdown"
   },
   "source": [
    "## 1. Environment Setup and Library Configuration\n",
    "\n",
    "**Purpose:** This section prepares the Python environment for executing the data collection script. It involves importing essential libraries and configuring system paths to ensure that custom modules, particularly the `DataCollector` class, are accessible.\n",
    "\n",
    "**Key Actions:**\n",
    "*   **Import Core Libraries:** Standard Python libraries such as `sys` (for system-specific parameters and functions), `os` (for operating system interfaces), `pandas` (for data manipulation and analysis), and `datetime` (for timestamping) are imported.\n",
    "*   **Path Configuration:** The system path (`sys.path`) is dynamically updated to include the project's `src` directory. This is crucial for importing the `DataCollector` class and any utility functions residing in custom modules, assuming the notebook is executed from the `notebooks` directory.\n",
    "*   **Custom Module Import:** The `DataCollector` class, which encapsulates the logic for API interactions and data aggregation, is imported from the `src.data_collector` module.\n",
    "*   **Pandas Display Options:** `pandas` display settings are configured to enhance the readability of DataFrames, ensuring that all columns are visible and the output width is sufficient for comprehensive data inspection.\n",
    "*   **Execution Timestamp:** The start time of the notebook execution is recorded and printed, providing a reference point for monitoring the duration of the data collection process.\n",
    "\n",
    "**Significance:** Proper setup is foundational for the notebook's successful operation. It guarantees that all dependencies are loaded and that the `DataCollector` can be instantiated and utilized correctly, preventing import errors and ensuring a smooth data collection workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43842a4",
   "metadata": {
    "id": "56b64a94",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src directory to path to import modules\n",
    "# Assumes notebook is run from the 'notebooks' directory\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our custom modules\n",
    "from src.data_collector import DataCollector\n",
    "from src.utils import configure_plotting # Optional: if plotting is needed here\n",
    "\n",
    "# Configure plotting (optional)\n",
    "configure_plotting()\n",
    "\n",
    "# Display pandas DataFrames nicely\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Display current time for reference\n",
    "print(f\"Notebook Execution Started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6729227d",
   "metadata": {
    "id": "34393ae7",
    "language": "markdown"
   },
   "source": [
    "## 2. Data Collector Instantiation\n",
    "\n",
    "**Purpose:** This step involves creating an instance of the `DataCollector` class. This object will serve as the primary interface for all subsequent data collection operations.\n",
    "\n",
    "**Functionality of `DataCollector`:**\n",
    "*   **API Key Management:** The `DataCollector` is designed to automatically load necessary API keys and credentials from a `.env` file located in the project's root directory. This promotes security and ease of configuration.\n",
    "*   **Game List Management:** It initializes with predefined lists of game titles and their corresponding App IDs, categorized for targeted data retrieval (e.g., 'successful', 'declining', 'experimental').\n",
    "*   **Data Storage Configuration:** The collector is configured with a `data_dir` parameter, specifying the directory (relative to the notebook, e.g., `../data`) where collected data will be saved.\n",
    "*   **Centralized Operations:** It encapsulates methods for fetching data from various sources (Steam, Twitch, external platforms) and for saving the aggregated data.\n",
    "\n",
    "**Expected Output Upon Execution:**\n",
    "*   Successful instantiation of the `DataCollector` object without errors.\n",
    "*   A printed confirmation indicating the total number of unique game IDs being tracked across all defined categories. This verifies that the game lists have been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb9604",
   "metadata": {
    "id": "16d376da",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Initialize the collector\n",
    "# It will use the default game lists defined within the class\n",
    "# and look for API keys in the .env file in the project root\n",
    "# Ensure the data_dir path is correct relative to the notebook location\n",
    "collector = DataCollector(data_dir=\"../data\")\n",
    "\n",
    "# Optionally, view the game IDs being tracked\n",
    "all_game_ids = collector.get_all_game_ids()\n",
    "print(f\"Tracking {len(all_game_ids)} unique game IDs across categories.\")\n",
    "print(collector.game_categories) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4dd867",
   "metadata": {
    "id": "7fb97b85",
    "language": "markdown"
   },
   "source": [
    "## 3. Execution of Real-Time Data Collection\n",
    "\n",
    "**Purpose:** This section executes the core data collection logic by invoking the `collect_current_data` method of the instantiated `DataCollector` object.\n",
    "\n",
    "**Process Overview:** The method systematically queries multiple APIs and web sources to gather a comprehensive set of metrics for each tracked game. The scope of data collection can be controlled via parameters:\n",
    "*   `include_details=True`: Fetches detailed game information from Steam, which is often a prerequisite for subsequent lookups on platforms like Twitch and other external sources that rely on game names rather than App IDs.\n",
    "*   `include_twitch=True`: Retrieves current viewership numbers for each game on Twitch.\n",
    "*   `include_external=True`: Gathers data from Google Trends (search interest), Reddit (community engagement), Twitter (social buzz), and YouTube (video statistics).\n",
    "\n",
    "**Guidelines for Successful Operation & API Usage Considerations:**\n",
    "1.  **API Key/Credential Configuration:** For seamless data retrieval, ensure that all API keys, secrets, and tokens are correctly configured in the `.env` file (located in the project root: `c:\\\\Users\\\\lucav\\\\Github\\\\Game-Popularity-Prediction-Modelv2`). Accuracy is particularly important for **Reddit** (Client ID, Secret, User Agent) and **YouTube** (API Key) credentials to prevent access issues (`401` or `403` status codes).\n",
    "2.  **Adherence to API Quotas and Rate Limits:** Be mindful of the usage limits imposed by APIs such as YouTube, Google Trends, and Twitter. These services often have quotas (e.g., total requests per day) and rate limits (requests per time interval). To ensure continuous operation and avoid service interruptions (which can result in `403` - Quota Exceeded or `429` - Too Many Requests statuses), monitor usage, especially when collecting data for many games or running the notebook frequently. Consulting the API provider dashboards (e.g., Google Cloud Console for YouTube API) can help manage these constraints.\n",
    "\n",
    "**Expected Output Upon Execution:**\n",
    "*   **Progress Indicators:** Status messages will be displayed, indicating which data sources are currently being queried (e.g., \"Fetching Steam player counts...\", \"Fetching Twitch data...\").\n",
    "*   **Error/Warning Notifications:** Any issues encountered during the process, such as API errors (due to invalid keys, quota limits, or network problems) or missing data for specific games, will be reported.\n",
    "*   **Resultant DataFrame:** If the collection is at least partially successful, a `pandas` DataFrame (`current_data_df`) will be generated. This DataFrame will contain the aggregated metrics, with columns corresponding to each data point (e.g., `app_id`, `name`, `player_count`, `twitch_viewer_count`, `google_trends_avg`).\n",
    "*   **Data Summary:** A summary of the collected DataFrame, including its shape (number of rows and columns) and a list of all column names, will be printed to confirm the structure of the retrieved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04aebd",
   "metadata": {
    "id": "17b0d869",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Collect data for all tracked games\n",
    "# include_details=True is needed to get game names for Twitch/External lookups\n",
    "# include_twitch=True fetches Twitch viewership\n",
    "# include_external=True fetches Google Trends, Reddit, Twitter, YouTube data\n",
    "print(\"Starting data collection...\")\n",
    "print(\"This may take a few minutes depending on the number of games and API responsiveness.\")\n",
    "try:\n",
    "    current_data_df = collector.collect_current_data(\n",
    "        include_details=True,\n",
    "        include_twitch=True,\n",
    "        include_external=True\n",
    "    )\n",
    "    print(\"\\n--- Collected Data Sample ---\")\n",
    "    # Display relevant columns, especially the newly added ones\n",
    "    display_cols = [\n",
    "        'app_id', 'name', 'category', 'timestamp', 'player_count', 'twitch_viewer_count',\n",
    "        'google_trends_avg', 'reddit_subscribers', 'reddit_active_users', 'reddit_recent_posts', 'twitter_recent_count',\n",
    "        'youtube_total_views', 'youtube_avg_views', 'youtube_avg_likes', 'release_date'\n",
    "    ]\n",
    "    display_cols_present = [col for col in display_cols if col in current_data_df.columns]\n",
    "    display(current_data_df[display_cols_present].head())\n",
    "    print(f\"\\nShape: {current_data_df.shape}\")\n",
    "    print(\"\\nColumns:\", current_data_df.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during data collection: {e}\")\n",
    "    # Optionally re-raise if debugging: raise e\n",
    "    current_data_df = pd.DataFrame() # Ensure df exists but is empty on error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6002d98",
   "metadata": {
    "id": "75faf4a7",
    "language": "markdown"
   },
   "source": [
    "## 4. Data Persistence: Saving Collected Metrics\n",
    "\n",
    "**Purpose:** This section is responsible for saving the `current_data_df` DataFrame, which contains the newly collected game metrics, to a persistent file format.\n",
    "\n",
    "**Methodology:**\n",
    "*   **Conditional Save:** The save operation is performed only if the `current_data_df` exists and is not empty, preventing errors from attempting to save null or failed collection attempts.\n",
    "*   **`save_data` Method:** The `DataCollector`'s `save_data` method is utilized. This method handles the specifics of file naming (typically incorporating a timestamp to ensure unique filenames for each collection run) and serialization.\n",
    "*   **Compression:** The `compress=True` argument is passed to the `save_data` method, indicating that the output CSV file should be compressed (e.g., using gzip). This is beneficial for reducing storage space, especially as the historical dataset grows.\n",
    "*   **File Path:** The data is saved within the `data_dir` (e.g., `../data`) specified during the `DataCollector` instantiation.\n",
    "\n",
    "**Significance:** Persisting the collected data is crucial for building a longitudinal dataset. Each execution of this notebook contributes a new snapshot of game metrics, which will be aggregated and processed in later stages (e.g., by a `DataAggregator` in `02_feature_engineering.ipynb`) to create a comprehensive time-series dataset suitable for model training and analysis.\n",
    "\n",
    "**Expected Output Upon Execution:**\n",
    "*   If data is successfully saved, a confirmation message will be printed, indicating the full file path of the saved (and compressed) CSV file.\n",
    "*   If no data was collected or an error occurred during collection, a message indicating that the save operation is being skipped will be displayed.\n",
    "*   Potential error messages if the save operation itself encounters an issue (e.g., disk full, permissions error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ac21c",
   "metadata": {
    "id": "eba635e7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Save the collected data to a compressed CSV file\n",
    "try:\n",
    "    # Check if the DataFrame exists and is not empty\n",
    "    if 'current_data_df' in locals() and not current_data_df.empty:\n",
    "        saved_filepath = collector.save_data(data=current_data_df, compress=True)\n",
    "        print(f\"\\nData successfully saved to: {saved_filepath}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nSkipping save: No data collected or collection failed.\")\n",
    "except Exception as e:\n",
    "     print(f\"\\nAn error occurred while saving data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2f524",
   "metadata": {
    "id": "2d4debca",
    "language": "markdown"
   },
   "source": [
    "## 5. Preliminary Data Inspection (Optional)\n",
    "\n",
    "**Purpose:** This optional step involves conducting a cursory examination of the `current_data_df` DataFrame. The primary goal is to quickly identify any conspicuous issues or anomalies in the freshly collected data.\n",
    "\n",
    "**Methodology:**\n",
    "*   **Conditional Execution:** The review is performed only if data collection was successful and `current_data_df` is not empty.\n",
    "*   **Descriptive Statistics:** The `describe()` method from `pandas` is applied to a predefined list of key numerical columns. This generates summary statistics, including count, mean, standard deviation, minimum, maximum, and quartile values for these metrics.\n",
    "*   **Selected Metrics for Review:** The statistics are typically generated for columns such as `player_count`, `twitch_viewer_count`, `google_trends_avg`, `reddit_subscribers`, `reddit_active_users`, `twitter_recent_count`, and various YouTube engagement metrics.\n",
    "\n",
    "**Significance:** A quick review of basic statistics can help in early detection of potential problems, such as:\n",
    "*   **Missing Data:** Unusually low counts in certain columns might indicate failures in specific API calls or data parsing.\n",
    "*   **Data Range Issues:** Unexpected minimum or maximum values could point to outliers or errors in data retrieval (e.g., negative player counts, excessively high viewership).\n",
    "*   **Consistency Checks:** Comparing metrics across different games or against known benchmarks can provide a sanity check.\n",
    "This initial check is not a substitute for thorough exploratory data analysis (EDA) but serves as a first-pass quality control measure.\n",
    "\n",
    "**Expected Output Upon Execution:**\n",
    "*   If data is available, a `pandas` DataFrame displaying descriptive statistics for the selected key numerical columns.\n",
    "*   If no data was collected or the DataFrame is empty, a message indicating that the statistics generation is being skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56e5bb",
   "metadata": {
    "id": "10686c60",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Display basic statistics for numerical columns if data was collected\n",
    "if 'current_data_df' in locals() and not current_data_df.empty:\n",
    "    print(\"\\nBasic Statistics for Key Metrics:\")\n",
    "    stats_cols = [\n",
    "        'player_count', 'twitch_viewer_count', 'google_trends_avg', \n",
    "        'reddit_subscribers', 'reddit_active_users', 'reddit_recent_posts', \n",
    "        'twitter_recent_count', 'youtube_total_views', 'youtube_avg_views', 'youtube_avg_likes'\n",
    "    ]\n",
    "    stats_cols_present = [col for col in stats_cols if col in current_data_df.columns]\n",
    "    display(current_data_df[stats_cols_present].describe())\n",
    "else:\n",
    "    print(\"\\nSkipping statistics: No data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6e99f",
   "metadata": {
    "id": "d1376301",
    "language": "markdown"
   },
   "source": [
    "## 6. Concluding Remarks and Future Work\n",
    "\n",
    "**Purpose:** This section outlines the subsequent phases of the project, building upon the data collection framework established in this notebook.\n",
    "\n",
    "**Rationale:** The data collected herein is the foundational element for developing the game popularity prediction model. The following actions are critical for progressing towards this objective:\n",
    "\n",
    "**Immediate and Long-Term Next Actions:**\n",
    "1.  **Iterative Data Collection:** To construct a robust time-series dataset, this notebook (`01_data_collection.ipynb`) must be executed periodically (e.g., daily or weekly, depending on the desired granularity and API constraints). Each execution will append a new snapshot of game metrics to the historical data store.\n",
    "2.  **Data Aggregation and Preprocessing:** Once a sufficient volume of historical data has been accumulated, the `DataAggregator` utility (anticipated to be a key component in `02_feature_engineering.ipynb`) will be employed. Its role will be to:\n",
    "    *   Load and consolidate the individual, timestamped data files saved by this notebook.\n",
    "    *   Perform necessary cleaning, transformation, and alignment of the time-series data.\n",
    "    *   Potentially interpolate missing values or handle inconsistencies.\n",
    "3.  **Feature Engineering:** Following aggregation, `02_feature_engineering.ipynb` will focus on deriving meaningful features from the processed time-series data. This may include creating lagged variables, rolling averages, trend indicators, and other domain-specific features relevant to game popularity.\n",
    "4.  **Predictive Modeling:** The engineered features will then serve as input for the modeling phase, primarily detailed in `03_modeling.ipynb`. This will involve selecting appropriate machine learning algorithms (e.g., time-series models, regression models), training them on the historical feature set, and evaluating their performance in predicting future game popularity metrics.\n",
    "5.  **Model Evaluation and Refinement:** The performance of the developed models will be rigorously assessed, and iterative refinements to features, model selection, and hyperparameter tuning will be conducted to optimize predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91691b0",
   "metadata": {
    "id": "639c9ddf",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Final summary message\n",
    "print(\"\\nData Collection Notebook Complete.\")\n",
    "if 'saved_filepath' in locals():\n",
    "    print(f\"Latest data saved to: {saved_filepath}\")\n",
    "elif 'current_data_df' in locals() and current_data_df.empty:\n",
    "     print(\"Data collection run finished, but resulted in empty data or an error occurred.\")\n",
    "else:\n",
    "     print(\"Data collection run finished, but data was not saved (likely due to an error). Check previous cell outputs.\")\n",
    "\n",
    "print(\"\\nRemember to run this notebook periodically to build your historical dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed549a",
   "metadata": {
    "id": "8432d70e",
    "language": "markdown"
   },
   "source": [
    "---\n",
    "*End of Notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
